"""
üéØ Multi-Objective Ranker –¥–ª—è LoveMemory AI
–§–∞–∑–∞ 2.1: –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–µ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ

–ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ—Å—Ç—å,
–Ω–æ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:

1. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (Relevance) - –Ω–∞—Å–∫–æ–ª—å–∫–æ —ç—Ç–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–∞—Ä–µ
2. –ù–æ–≤–∏–∑–Ω–∞ (Novelty) - –Ω–∞—Å–∫–æ–ª—å–∫–æ —ç—Ç–æ –Ω–æ–≤–æ –∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ  
3. –≠–º–ø–∞—Ç–∏—è (Empathy) - –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å—ã –æ–±–æ–∏—Ö –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤

–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è —Ü–µ–ª—å: —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–æ—á–Ω—ã, –Ω–æ –∏ –¥–∞—Ä—è—Ç —ç–º–æ—Ü–∏–∏
"""

import json
import os
import pickle
import time
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GroupKFold
from sklearn.metrics import ndcg_score
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

from content_recommender import ContentBasedRecommender
from collaborative_filtering import CollaborativeFilteringRecommender
from embedding_service import EmbeddingService
from personality_engine import PersonalityProfile, personality_engine

@dataclass
class MultiObjectiveScore:
    """–ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
    relevance: float  # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0-1)
    novelty: float    # –ù–æ–≤–∏–∑–Ω–∞ (0-1)
    empathy: float    # –≠–º–ø–∞—Ç–∏—è (0-1)
    combined: float   # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    
    def to_dict(self) -> Dict[str, float]:
        return {
            'relevance': self.relevance,
            'novelty': self.novelty,
            'empathy': self.empathy,
            'combined': self.combined
        }

class MultiObjectiveRanker:
    """
    –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π —Ä–∞–Ω–∫–µ—Ä - —Å–ª–µ–¥—É—é—â–µ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º
    
    –ù–µ –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –º–µ—Å—Ç–∞, –∞ —Å–æ–∑–¥–∞–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∑–Ω–∞—á–∏–º—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏,
    –∫–æ—Ç–æ—Ä—ã–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –¥–∏–Ω–∞–º–∏–∫—É –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞—é—Ç –Ω–æ–≤—ã–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è
    """
    
    def __init__(self, data_path: str = 'data/synthetic_v2_enhanced'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–Ω–∫–µ—Ä–∞
        
        Args:
            data_path: –ü—É—Ç—å –∫ enhanced –¥–∞–Ω–Ω—ã–º —Å OCEAN –ª–∏—á–Ω–æ—Å—Ç—è–º–∏
        """
        self.data_path = data_path
        print("üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Multi-Objective Ranker...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.content_recommender = ContentBasedRecommender(data_path)
        self.cf_recommender = CollaborativeFilteringRecommender(data_path)
        self.embedding_service = EmbeddingService(data_path)
        
        # –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å
        self.multi_objective_model = None
        self.feature_names = []
        self.feature_importance = {}
        
        # –í–µ—Å–∞ –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        self.objective_weights = {
            'relevance': 0.5,   # –û—Å–Ω–æ–≤–∞ - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            'novelty': 0.3,     # –í–∞–∂–Ω–∞ –Ω–æ–≤–∏–∑–Ω–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ä—É—Ç–∏–Ω—ã
            'empathy': 0.2      # –ë–∞–ª–∞–Ω—Å –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤
        }
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
        self.lgb_params = {
            'objective': 'lambdarank',
            'metric': 'ndcg',
            'boosting_type': 'gbdt',
            'num_leaves': 63,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
            'learning_rate': 0.03,  # –°–Ω–∏–∂–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'min_data_in_leaf': 20,
            'min_gain_to_split': 0.1,
            'verbose': -1
        }
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._prepare_components()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º enhanced –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        self._load_enhanced_data()
    
    def _prepare_components(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã"""
        print("‚öôÔ∏è –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Multi-Objective Ranker...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º CF –º–æ–¥–µ–ª—å
        if os.path.exists('models/cf_svd_v1.pkl'):
            self.cf_recommender.load_model('models/cf_svd_v1')
        else:
            print("üîÑ –¢—Ä–µ–Ω–∏—Ä—É–µ–º CF –º–æ–¥–µ–ª—å...")
            self.cf_recommender.train_svd_model()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        if not self.embedding_service.load_embeddings():
            print("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
            self.embedding_service.generate_user_embeddings()
            self.embedding_service.generate_product_embeddings()
            self.embedding_service.generate_pair_embeddings()
            self.embedding_service.build_faiss_indexes()
            self.embedding_service.save_embeddings()
        
        print("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã")
    
    def _load_enhanced_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç enhanced –¥–∞–Ω–Ω—ã–µ —Å OCEAN –ª–∏—á–Ω–æ—Å—Ç—è–º–∏"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å enhanced –¥–∞–Ω–Ω—ã–µ
            enhanced_users_path = f"{self.data_path}/users.csv"
            enhanced_pairs_path = f"{self.data_path}/pairs.csv"
            enhanced_interactions_path = f"{self.data_path}/interactions.csv"
            
            if all(os.path.exists(p) for p in [enhanced_users_path, enhanced_pairs_path, enhanced_interactions_path]):
                print("üìä –ó–∞–≥—Ä—É–∂–∞–µ–º Enhanced –¥–∞–Ω–Ω—ã–µ —Å OCEAN –ª–∏—á–Ω–æ—Å—Ç—è–º–∏...")
                self.enhanced_users = pd.read_csv(enhanced_users_path)
                self.enhanced_pairs = pd.read_csv(enhanced_pairs_path)
                self.enhanced_interactions = pd.read_csv(enhanced_interactions_path)
                print(f"‚úÖ Enhanced –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(self.enhanced_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, {len(self.enhanced_pairs)} –ø–∞—Ä")
            else:
                print("‚ö†Ô∏è Enhanced –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ")
                self.enhanced_users = None
                self.enhanced_pairs = None
                self.enhanced_interactions = None
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ enhanced –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.enhanced_users = None
            self.enhanced_pairs = None
            self.enhanced_interactions = None
    
    def calculate_relevance_score(self, pair_id: str, item_info: Dict, 
                                candidate_scores: Dict) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            item_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
            candidate_scores: Scores –æ—Ç –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
            
        Returns:
            –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0-1)
        """
        # –ë–∞–∑–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
        content_score = candidate_scores.get('content_score', 0.0)
        cf_score = candidate_scores.get('cf_score', 0.0)
        embedding_score = candidate_scores.get('embedding_score', 0.0)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤–µ—Å–∞)
        base_relevance = (
            content_score * 0.4 +
            cf_score * 0.3 +
            embedding_score * 0.3
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevance_boost = 0
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä–µ
            if self.enhanced_pairs is not None:
                pair_info = self.enhanced_pairs[self.enhanced_pairs['id'] == pair_id]
                if not pair_info.empty:
                    harmony_index = pair_info.iloc[0].get('harmony_index', 0.5)
                    # –ì–∞—Ä–º–æ–Ω–∏—á–Ω—ã–µ –ø–∞—Ä—ã –ø–æ–ª—É—á–∞—é—Ç –±–æ–Ω—É—Å –∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    relevance_boost += harmony_index * 0.1
            
            # –ë—é–¥–∂–µ—Ç–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            if 'price' in item_info:
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—é–¥–∂–µ—Ç–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                price = item_info['price']
                if 500 <= price <= 2500:  # –†–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
                    relevance_boost += 0.05
                elif price > 3000:  # –î–æ—Ä–æ–≥–æ
                    relevance_boost -= 0.1
        
        except Exception as e:
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –≤ —Ä–∞—Å—á–µ—Ç–µ –±–æ–Ω—É—Å–æ–≤
            pass
        
        final_relevance = min(1.0, max(0.0, base_relevance + relevance_boost))
        return final_relevance
    
    def calculate_novelty_score(self, pair_id: str, item_info: Dict) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –Ω–æ–≤–∏–∑–Ω—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        
    –ù–æ–≤–∏–∑–Ω–∞ –≤–∞–∂–Ω–∞ –¥–ª—è:
        - –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ä—É—Ç–∏–Ω—ã –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö
        - –°–æ–∑–¥–∞–Ω–∏—è —è—Ä–∫–∏—Ö –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–π
        - –û—Ç–∫—Ä—ã—Ç–∏—è –Ω–æ–≤—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            item_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
            
        Returns:
            –ù–æ–≤–∏–∑–Ω–∞ (0-1)
        """
        novelty_score = 0.5  # –ë–∞–∑–æ–≤–∞—è –Ω–æ–≤–∏–∑–Ω–∞
        
        try:
            # 1. –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –Ω–æ–≤–∏–∑–Ω–∞ —Ç–æ–≤–∞—Ä–∞
            item_novelty = item_info.get('novelty', 0.5)
            novelty_score += item_novelty * 0.3
            
            # 2. –ù–æ–≤–∏–∑–Ω–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä—ã
            if self.enhanced_interactions is not None:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Å–µ—â–∞–ª–∞ –ª–∏ –ø–∞—Ä–∞ —ç—Ç–æ –º–µ—Å—Ç–æ
                pair_interactions = self.enhanced_interactions[
                    self.enhanced_interactions['pair_id'] == pair_id
                ]
                
                item_title = item_info.get('title', '')
                visited_count = len(pair_interactions[
                    pair_interactions['product_id'] == item_title
                ])
                
                if visited_count == 0:
                    novelty_score += 0.3  # –ù–æ–≤–æ–µ –º–µ—Å—Ç–æ –¥–ª—è –ø–∞—Ä—ã
                elif visited_count == 1:
                    novelty_score += 0.1  # –ë—ã–ª–æ –æ–¥–∏–Ω —Ä–∞–∑ - –º–æ–∂–µ—Ç —Å—Ç–æ–∏—Ç –ø–æ–≤—Ç–æ—Ä–∏—Ç—å
                else:
                    novelty_score -= 0.2 * min(visited_count - 1, 3)  # –®—Ç—Ä–∞—Ñ –∑–∞ —á–∞—Å—Ç—ã–µ –ø–æ—Å–µ—â–µ–Ω–∏—è
                
                # 3. –ù–æ–≤–∏–∑–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–∞—Ä—ã
                item_category = item_info.get('category', '')
                category_interactions = pair_interactions[
                    pair_interactions['product_category'] == item_category
                ]
                
                if len(category_interactions) == 0:
                    novelty_score += 0.2  # –ù–æ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            
            # 4. –°–µ–∑–æ–Ω–Ω–∞—è/–≤—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–æ–≤–∏–∑–Ω–∞
            current_season = self._get_current_season()
            item_tags = item_info.get('tags', [])
            
            seasonal_keywords = {
                'winter': ['–Ω–æ–≤–æ–≥–æ–¥–Ω–∏–π', '–∑–∏–º–Ω–∏–π', '–≥–æ—Ä—è—á–∏–π'],
                'spring': ['–≤–µ—Å–µ–Ω–Ω–∏–π', '—Å–≤–µ–∂–∏–π', '—Ü–≤–µ—Ç—É—â–∏–π'],
                'summer': ['–ª–µ—Ç–Ω–∏–π', '–ø—Ä–æ—Ö–ª–∞–¥–Ω—ã–π', '–æ—Ç–∫—Ä—ã—Ç—ã–π'],
                'autumn': ['–æ—Å–µ–Ω–Ω–∏–π', '—É—é—Ç–Ω—ã–π', '—Ç–µ–ø–ª—ã–π']
            }
            
            season_keywords = seasonal_keywords.get(current_season, [])
            for keyword in season_keywords:
                if any(keyword in tag.lower() for tag in item_tags):
                    novelty_score += 0.1
                    break
            
            # 5. –¢—Ä–µ–Ω–¥—ã –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å (—Å–∏–º—É–ª—è—Ü–∏—è)
            trending_categories = ['entertainment', 'activity']  # –°–∏–º—É–ª—è—Ü–∏—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if item_category in trending_categories:
                novelty_score += 0.1
                
        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –Ω–æ–≤–∏–∑–Ω—É
            pass
        
        return min(1.0, max(0.0, novelty_score))
    
    def calculate_empathy_score(self, pair_id: str, item_info: Dict) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —ç–º–ø–∞—Ç–∏—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—É–Ω–∏–∫–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        
        –≠–º–ø–∞—Ç–∏—è –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —É—á–∏—Ç—ã–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å—ã –û–ë–û–ò–• –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤
        –∏ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –≥–∞—Ä–º–æ–Ω–∏–∏ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö.
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            item_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
            
        Returns:
            –≠–º–ø–∞—Ç–∏—è (0-1)
        """
        empathy_score = 0.3  # –ë–∞–∑–æ–≤–∞—è —ç–º–ø–∞—Ç–∏—è
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä–µ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
            if self.enhanced_pairs is not None and self.enhanced_users is not None:
                pair_info = self.enhanced_pairs[self.enhanced_pairs['id'] == pair_id]
                if pair_info.empty:
                    return 0.5
                
                pair_row = pair_info.iloc[0]
                user1_id = pair_row['user1_id']
                user2_id = pair_row['user2_id']
                
                user1 = self.enhanced_users[self.enhanced_users['id'] == user1_id]
                user2 = self.enhanced_users[self.enhanced_users['id'] == user2_id]
                
                if user1.empty or user2.empty:
                    return 0.5
                
                user1_row = user1.iloc[0]
                user2_row = user2.iloc[0]
                
                # 1. –ë–∞–ª–∞–Ω—Å –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤
                item_tags = item_info.get('tags', [])
                
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ç–µ—Ä–µ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (—Ä–∞–±–æ—Ç–∞–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏)
                user1_interests = self._parse_interests(user1_row.get('interests', '{}'))
                user2_interests = self._parse_interests(user2_row.get('interests', '{}'))
                
                user1_appeal = self._calculate_interest_appeal(user1_interests, item_tags)
                user2_appeal = self._calculate_interest_appeal(user2_interests, item_tags)
                
                # –í—ã—Å–æ–∫–∞—è —ç–º–ø–∞—Ç–∏—è –∫–æ–≥–¥–∞ –æ–±–∞ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω—ã
                if user1_appeal > 0.5 and user2_appeal > 0.5:
                    empathy_score += 0.3
                elif abs(user1_appeal - user2_appeal) < 0.2:  # –°—Ö–æ–∂–∏–π —É—Ä–æ–≤–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–∞
                    empathy_score += 0.2
                elif max(user1_appeal, user2_appeal) > 0.8:  # –û–¥–∏–Ω –æ—á–µ–Ω—å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω
                    empathy_score += 0.1
                
                # 2. –£—á–µ—Ç —è–∑—ã–∫–æ–≤ –ª—é–±–≤–∏
                item_love_language = item_info.get('love_language', '')
                
                user1_love_langs = self._parse_love_languages(user1_row.get('love_languages', '{}'))
                user2_love_langs = self._parse_love_languages(user2_row.get('love_languages', '{}'))
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —è–∑—ã–∫–∞–º –ª—é–±–≤–∏
                user1_lang_match = user1_love_langs.get(item_love_language, 0.0)
                user2_lang_match = user2_love_langs.get(item_love_language, 0.0)
                
                avg_lang_match = (user1_lang_match + user2_lang_match) / 2
                empathy_score += avg_lang_match * 0.2
                
                # 3. –ö–æ–º–ø—Ä–æ–º–∏—Å—Å–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏—á–Ω–æ—Å—Ç–∏
                try:
                    user1_personality = self._parse_personality(user1_row.get('personality', '{}'))
                    user2_personality = self._parse_personality(user2_row.get('personality', '{}'))
                    
                    # –î–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –ª—é–¥–∏ –±–æ–ª—å—à–µ –∏–¥—É—Ç –Ω–∞ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã
                    avg_agreeableness = (
                        user1_personality.get('agreeableness', 0.5) +
                        user2_personality.get('agreeableness', 0.5)
                    ) / 2
                    
                    # –ï—Å–ª–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç—Ä–µ–±—É–µ—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞, —É—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                    if abs(user1_appeal - user2_appeal) > 0.3:  # –†–∞–∑–Ω—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã
                        empathy_score += avg_agreeableness * 0.15
                        
                except:
                    pass
                
                # 4. –ì–∞—Ä–º–æ–Ω–∏—è –ø–∞—Ä—ã
                harmony_index = pair_row.get('harmony_index', 0.5)
                if harmony_index > 0.7:  # –ì–∞—Ä–º–æ–Ω–∏—á–Ω—ã–µ –ø–∞—Ä—ã
                    empathy_score += 0.1
                
                # 5. –ü–æ–¥—Ö–æ–¥—è—â–∞—è —Ü–µ–Ω–∞ –¥–ª—è –æ–±–æ–∏—Ö
                item_price = item_info.get('price', 1000)
                user1_budget = user1_row.get('budget_preference', 'medium')
                user2_budget = user2_row.get('budget_preference', 'medium')
                
                budget_ranges = {'low': 1000, 'medium': 2500, 'high': 5000}
                user1_max = budget_ranges.get(user1_budget, 2500)
                user2_max = budget_ranges.get(user2_budget, 2500)
                
                if item_price <= min(user1_max, user2_max):
                    empathy_score += 0.1  # –î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –æ–±–æ–∏—Ö
                elif item_price <= max(user1_max, user2_max):
                    empathy_score += 0.05  # –î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –æ–¥–Ω–æ–≥–æ
                
        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é —ç–º–ø–∞—Ç–∏—é
            pass
        
        return min(1.0, max(0.0, empathy_score))
    
    def _parse_interests(self, interests_str: str) -> Dict[str, float]:
        """–ü–∞—Ä—Å–∏—Ç –∏–Ω—Ç–µ—Ä–µ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        try:
            if isinstance(interests_str, str):
                return eval(interests_str) if interests_str else {}
            elif isinstance(interests_str, dict):
                return interests_str
            else:
                return {}
        except:
            return {}
    
    def _parse_love_languages(self, love_langs_str: str) -> Dict[str, float]:
        """–ü–∞—Ä—Å–∏—Ç —è–∑—ã–∫–∏ –ª—é–±–≤–∏ –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        try:
            if isinstance(love_langs_str, str):
                return eval(love_langs_str) if love_langs_str else {}
            elif isinstance(love_langs_str, dict):
                return love_langs_str
            else:
                return {}
        except:
            return {}
    
    def _parse_personality(self, personality_str: str) -> Dict[str, float]:
        """–ü–∞—Ä—Å–∏—Ç –ª–∏—á–Ω–æ—Å—Ç—å OCEAN –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        try:
            if isinstance(personality_str, str):
                return eval(personality_str) if personality_str else {}
            elif isinstance(personality_str, dict):
                return personality_str
            else:
                return {}
        except:
            return {}
    
    def _calculate_interest_appeal(self, interests: Dict[str, float], tags: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤"""
        if not interests or not tags:
            return 0.3
        
        total_appeal = 0
        matches = 0
        
        for tag in tags:
            for interest, intensity in interests.items():
                if tag.lower() in interest.lower() or interest.lower() in tag.lower():
                    total_appeal += intensity / 10.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å
                    matches += 1
        
        if matches == 0:
            return 0.3
        
        return min(1.0, total_appeal / matches)
    
    def _get_current_season(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â–∏–π —Å–µ–∑–æ–Ω"""
        month = datetime.now().month
        if month in [12, 1, 2]:
            return 'winter'
        elif month in [3, 4, 5]:
            return 'spring'
        elif month in [6, 7, 8]:
            return 'summer'
        else:
            return 'autumn'
    
    def calculate_multi_objective_score(self, pair_id: str, item_info: Dict, 
                                      candidate_scores: Dict) -> MultiObjectiveScore:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ–ª–Ω—É—é –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—É—é –æ—Ü–µ–Ω–∫—É
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            item_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
            candidate_scores: –ë–∞–∑–æ–≤—ã–µ scores –æ—Ç –º–æ–¥–µ–ª–µ–π
            
        Returns:
            MultiObjectiveScore —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π
        """
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞–∂–¥—É—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
        relevance = self.calculate_relevance_score(pair_id, item_info, candidate_scores)
        novelty = self.calculate_novelty_score(pair_id, item_info)
        empathy = self.calculate_empathy_score(pair_id, item_info)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞–∫ –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        combined = (
            relevance * self.objective_weights['relevance'] +
            novelty * self.objective_weights['novelty'] +
            empathy * self.objective_weights['empathy']
        )
        
        return MultiObjectiveScore(
            relevance=round(relevance, 3),
            novelty=round(novelty, 3),
            empathy=round(empathy, 3),
            combined=round(combined, 3)
        )
    
    def extract_enhanced_features(self, pair_id: str, item_id: str, candidate: Dict) -> Dict[str, float]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–∏—á –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        
        –í–∫–ª—é—á–∞–µ—Ç –≤—Å–µ –±–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ + –Ω–æ–≤—ã–µ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        """
        features = {}
        
        # 1. –ë–∞–∑–æ–≤—ã–µ scores –æ—Ç —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        features['content_score'] = candidate.get('content_score', 0.0)
        features['cf_score'] = candidate.get('cf_score', 0.0)
        features['embedding_score'] = candidate.get('embedding_score', 0.0)
        
        # 2. –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–µ scores
        item_info = {
            'title': candidate.get('title', ''),
            'category': candidate.get('category', 'unknown'),
            'price': candidate.get('price', 1000),
            'tags': candidate.get('tags', []),
            'love_language': candidate.get('love_language', 'quality_time'),
            'novelty': candidate.get('novelty', 0.5)
        }
        
        multi_scores = self.calculate_multi_objective_score(pair_id, item_info, candidate)
        features['relevance_score'] = multi_scores.relevance
        features['novelty_score'] = multi_scores.novelty
        features['empathy_score'] = multi_scores.empathy
        
        # 3. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
        features['price'] = item_info['price'] / 1000.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        features['price_log'] = np.log1p(item_info['price'])
        features['item_novelty'] = item_info.get('novelty', 0.5)
        
        # 4. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏
        category = item_info['category']
        features['is_restaurant'] = 1.0 if category == 'restaurant' else 0.0
        features['is_cafe'] = 1.0 if category == 'cafe' else 0.0
        features['is_entertainment'] = 1.0 if category == 'entertainment' else 0.0
        features['is_activity'] = 1.0 if category == 'activity' else 0.0
        features['is_bar'] = 1.0 if category == 'bar' else 0.0
        features['is_gift'] = 1.0 if category == 'gift' else 0.0
        
        # 5. –§–∏—á–∏ –ø–∞—Ä—ã –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        pair_features = self._get_enhanced_pair_features(pair_id)
        features.update(pair_features)
        
        # 6. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏
        features['day_of_week'] = datetime.now().weekday()
        features['is_weekend'] = 1.0 if datetime.now().weekday() >= 5 else 0.0
        features['hour_of_day'] = datetime.now().hour / 24.0
        
        # 7. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏
        features['relevance_novelty_product'] = features['relevance_score'] * features['novelty_score']
        features['empathy_relevance_product'] = features['empathy_score'] * features['relevance_score']
        features['all_objectives_product'] = features['relevance_score'] * features['novelty_score'] * features['empathy_score']
        
        # 8. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–µ —Ñ–∏—á–∏
        features['novelty_vs_relevance_ratio'] = features['novelty_score'] / max(0.1, features['relevance_score'])
        features['empathy_vs_relevance_ratio'] = features['empathy_score'] / max(0.1, features['relevance_score'])
        
        # 9. –¶–µ–Ω–∞ vs —Ü–µ–ª–∏
        features['price_empathy_interaction'] = features['price'] * features['empathy_score']
        features['novelty_price_interaction'] = features['novelty_score'] / max(0.1, features['price'])
        
        return features
    
    def _get_enhanced_pair_features(self, pair_id: str) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –ø–∞—Ä—ã —Å OCEAN –ª–∏—á–Ω–æ—Å—Ç—è–º–∏"""
        features = {}
        
        try:
            if self.enhanced_pairs is not None and self.enhanced_users is not None:
                pair_info = self.enhanced_pairs[self.enhanced_pairs['id'] == pair_id]
                if not pair_info.empty:
                    pair_row = pair_info.iloc[0]
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
                    features['harmony_index'] = pair_row.get('harmony_index', 0.5)
                    features['routine_index'] = pair_row.get('routine_index', 0.0)
                    features['adventure_appetite'] = pair_row.get('adventure_appetite', 0.5)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                    user1_id = pair_row['user1_id']
                    user2_id = pair_row['user2_id']
                    
                    user1 = self.enhanced_users[self.enhanced_users['id'] == user1_id]
                    user2 = self.enhanced_users[self.enhanced_users['id'] == user2_id]
                    
                    if not user1.empty and not user2.empty:
                        user1_row = user1.iloc[0]
                        user2_row = user2.iloc[0]
                        
                        # –ë–∞–∑–æ–≤—ã–µ –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏
                        features['age_diff'] = abs(user1_row['age'] - user2_row['age']) / 50.0
                        features['avg_age'] = (user1_row['age'] + user2_row['age']) / 2.0 / 50.0
                        
                        # –ê—Ä—Ö–µ—Ç–∏–ø—ã
                        features['same_archetype'] = 1.0 if user1_row['archetype'] == user2_row['archetype'] else 0.0
                        
                        # OCEAN –ª–∏—á–Ω–æ—Å—Ç—å —Ñ–∏—á–∏
                        try:
                            user1_personality = self._parse_personality(user1_row.get('personality', '{}'))
                            user2_personality = self._parse_personality(user2_row.get('personality', '{}'))
                            
                            # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è OCEAN
                            features['avg_openness'] = (user1_personality.get('openness', 0.5) + 
                                                       user2_personality.get('openness', 0.5)) / 2
                            features['avg_conscientiousness'] = (user1_personality.get('conscientiousness', 0.5) + 
                                                                user2_personality.get('conscientiousness', 0.5)) / 2
                            features['avg_extraversion'] = (user1_personality.get('extraversion', 0.5) + 
                                                           user2_personality.get('extraversion', 0.5)) / 2
                            features['avg_agreeableness'] = (user1_personality.get('agreeableness', 0.5) + 
                                                            user2_personality.get('agreeableness', 0.5)) / 2
                            features['avg_neuroticism'] = (user1_personality.get('neuroticism', 0.5) + 
                                                          user2_personality.get('neuroticism', 0.5)) / 2
                            
                            # –†–∞–∑–ª–∏—á–∏—è –≤ OCEAN (–≤–∞–∂–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                            features['openness_diff'] = abs(user1_personality.get('openness', 0.5) - 
                                                           user2_personality.get('openness', 0.5))
                            features['agreeableness_diff'] = abs(user1_personality.get('agreeableness', 0.5) - 
                                                               user2_personality.get('agreeableness', 0.5))
                            
                            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
                            features['both_open'] = 1.0 if (user1_personality.get('openness', 0.5) > 0.7 and 
                                                           user2_personality.get('openness', 0.5) > 0.7) else 0.0
                            features['both_agreeable'] = 1.0 if (user1_personality.get('agreeableness', 0.5) > 0.7 and 
                                                                user2_personality.get('agreeableness', 0.5) > 0.7) else 0.0
                            
                        except:
                            # –§–æ–ª–±—ç–∫ –∑–Ω–∞—á–µ–Ω–∏—è
                            for key in ['avg_openness', 'avg_conscientiousness', 'avg_extraversion', 
                                       'avg_agreeableness', 'avg_neuroticism', 'openness_diff', 
                                       'agreeableness_diff', 'both_open', 'both_agreeable']:
                                features[key] = 0.5
                        
                        # –ë—é–¥–∂–µ—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                        budget_match = 1.0 if user1_row['budget_preference'] == user2_row['budget_preference'] else 0.0
                        features['budget_match'] = budget_match
                        
                        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                        features['avg_activity'] = (user1_row.get('activity_probability', 0.5) + 
                                                   user2_row.get('activity_probability', 0.5)) / 2
                        
            # –§–æ–ª–±—ç–∫ –∑–Ω–∞—á–µ–Ω–∏—è –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
            for key in ['harmony_index', 'routine_index', 'adventure_appetite', 'age_diff', 'avg_age',
                       'same_archetype', 'budget_match', 'avg_activity']:
                if key not in features:
                    features[key] = 0.5
                    
        except Exception as e:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º —Ñ–æ–ª–±—ç–∫ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            default_features = {
                'harmony_index': 0.5, 'routine_index': 0.0, 'adventure_appetite': 0.5,
                'age_diff': 0.3, 'avg_age': 0.6, 'same_archetype': 0.0,
                'avg_openness': 0.5, 'avg_conscientiousness': 0.5, 'avg_extraversion': 0.5,
                'avg_agreeableness': 0.5, 'avg_neuroticism': 0.5,
                'openness_diff': 0.3, 'agreeableness_diff': 0.3,
                'both_open': 0.0, 'both_agreeable': 0.0,
                'budget_match': 0.5, 'avg_activity': 0.5
            }
            features.update(default_features)
        
        return features
    
    def create_multi_objective_training_dataset(self, sample_pairs: int = 300) -> Tuple[pd.DataFrame, List[int]]:
        """
        –°–æ–∑–¥–∞–µ—Ç training dataset –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            sample_pairs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            (features_df, groups) - —Ñ–∏—á–∏ –∏ –≥—Ä—É–ø–ø—ã –¥–ª—è LightGBM
        """
        print(f"üéØ –°–æ–∑–¥–∞–µ–º Multi-Objective training dataset –¥–ª—è {sample_pairs} –ø–∞—Ä...")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–∞—Ä—ã (preferably enhanced if available)
        if self.enhanced_pairs is not None:
            all_pairs = self.enhanced_pairs['id'].tolist()
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º Enhanced –¥–∞–Ω–Ω—ã–µ: {len(all_pairs)} –ø–∞—Ä –¥–æ—Å—Ç—É–ø–Ω–æ")
        else:
            all_pairs = self.content_recommender.pairs['id'].tolist()
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(all_pairs)} –ø–∞—Ä –¥–æ—Å—Ç—É–ø–Ω–æ")
        
        sample_pair_ids = np.random.choice(all_pairs, min(sample_pairs, len(all_pairs)), replace=False)
        
        training_data = []
        groups = []
        
        for pair_idx, pair_id in enumerate(sample_pair_ids):
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ª–æ–≥–∏–∫—É –Ω–æ —Å enhanced —Ñ–∏—á–∞–º–∏)
                candidates = self._get_candidates_for_pair(pair_id)
                
                if len(candidates) == 0:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –ª–µ–π–±–ª—ã –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                labels = self._get_multi_objective_labels(pair_id, candidates)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º enhanced —Ñ–∏—á–∏
                for candidate in candidates:
                    item_id = candidate['item_id']
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º enhanced —Ñ–∏—á–∏
                    features = self.extract_enhanced_features(pair_id, item_id, candidate)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –ª–µ–π–±–ª
                    label = labels.get(item_id, 0)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ dataset
                    row = {
                        'pair_id': pair_id,
                        'item_id': item_id,
                        'label': label,
                        **features
                    }
                    training_data.append(row)
                
                groups.append(len(candidates))
                
                if pair_idx % 50 == 0:
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–∞—Ä: {pair_idx + 1}/{len(sample_pair_ids)}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã {pair_id}: {e}")
                continue
        
        features_df = pd.DataFrame(training_data)
        
        print(f"‚úÖ Multi-Objective training dataset —Å–æ–∑–¥–∞–Ω:")
        print(f"  - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(features_df)}")
        print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø (–ø–∞—Ä): {len(groups)}")
        print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á: {len(features_df.columns) - 3}")
        print(f"  - –ù–æ–≤—ã—Ö –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö —Ñ–∏—á: relevance_score, novelty_score, empathy_score")
        
        return features_df, groups
    
    def _get_candidates_for_pair(self, pair_id: str, max_candidates: int = 40) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–±–∞–∑–æ–≤–∞—è –ª–æ–≥–∏–∫–∞)"""
        candidates = {}
        
        # Content-based –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        try:
            content_recs = self.content_recommender.recommend_date(pair_id, top_k=15)
            for rec in content_recs:
                candidates[rec.item_id] = {
                    'item_id': rec.item_id,
                    'title': rec.title,
                    'category': rec.category,
                    'price': rec.price,
                    'content_score': rec.score,
                    'tags': getattr(rec, 'tags', []),
                    'love_language': getattr(rec, 'love_language', 'quality_time'),
                    'novelty': getattr(rec, 'novelty', 0.5)
                }
        except:
            pass
        
        # CF –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        try:
            cf_recs = self.cf_recommender.get_pair_recommendations(pair_id, top_k=15)
            for rec in cf_recs:
                item_id = rec['item_id']
                if item_id in candidates:
                    candidates[item_id]['cf_score'] = rec['combined_rating'] / 10.0
                else:
                    product_info = self._get_product_info(item_id)
                    candidates[item_id] = {
                        'item_id': item_id,
                        'title': product_info.get('title', 'Unknown'),
                        'category': product_info.get('category', 'unknown'),
                        'price': product_info.get('price', 0),
                        'content_score': 0.0,
                        'cf_score': rec['combined_rating'] / 10.0,
                        'tags': product_info.get('tags', []),
                        'love_language': product_info.get('love_language', 'quality_time'),
                        'novelty': product_info.get('novelty', 0.5)
                    }
        except:
            pass
        
        # Embedding –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        try:
            embedding_recs = self.embedding_service.find_similar_products_ann(pair_id, top_k=15)
            for rec in embedding_recs:
                item_id = rec['item_id']
                if item_id in candidates:
                    candidates[item_id]['embedding_score'] = rec['embedding_similarity']
                else:
                    candidates[item_id] = {
                        'item_id': item_id,
                        'title': rec['title'],
                        'category': rec['category'],
                        'price': rec['price'],
                        'content_score': 0.0,
                        'cf_score': 0.0,
                        'embedding_score': rec['embedding_similarity'],
                        'tags': rec.get('tags', []),
                        'love_language': rec.get('love_language', 'quality_time'),
                        'novelty': rec.get('novelty', 0.5)
                    }
        except:
            pass
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ scores
        for candidate in candidates.values():
            candidate.setdefault('content_score', 0.0)
            candidate.setdefault('cf_score', 0.0)
            candidate.setdefault('embedding_score', 0.0)
        
        return list(candidates.values())[:max_candidates]
    
    def _get_multi_objective_labels(self, pair_id: str, candidates: List[Dict]) -> Dict[str, float]:
        """
        –°–æ–∑–¥–∞–µ—Ç –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–µ –ª–µ–π–±–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        –õ–µ–π–±–ª —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥, –Ω–æ –∏ –Ω–æ–≤–∏–∑–Ω—É/—ç–º–ø–∞—Ç–∏—é
        """
        labels = {}
        
        try:
            # –ë–∞–∑–æ–≤—ã–µ –ª–µ–π–±–ª—ã –æ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
            base_labels = self._get_base_labels_from_interactions(pair_id, candidates)
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            for candidate in candidates:
                item_id = candidate['item_id']
                
                # –ë–∞–∑–æ–≤—ã–π –ª–µ–π–±–ª
                base_label = base_labels.get(item_id, 0)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–µ scores
                item_info = {
                    'title': candidate.get('title', ''),
                    'category': candidate.get('category', ''),
                    'price': candidate.get('price', 1000),
                    'tags': candidate.get('tags', []),
                    'love_language': candidate.get('love_language', 'quality_time'),
                    'novelty': candidate.get('novelty', 0.5)
                }
                
                multi_scores = self.calculate_multi_objective_score(pair_id, item_info, candidate)
                
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –ª–µ–π–±–ª —Å –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–º–∏ scores
                enhanced_label = (
                    base_label * 0.6 +  # –ë–∞–∑–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                    multi_scores.combined * 3.0 * 0.4  # –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ (scale to 0-3)
                )
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –ª–µ–π–±–ª—ã –¥–ª—è LTR
                if enhanced_label >= 2.5:
                    labels[item_id] = 3  # –û—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                elif enhanced_label >= 1.8:
                    labels[item_id] = 2  # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                elif enhanced_label >= 1.2:
                    labels[item_id] = 1  # –°–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                else:
                    labels[item_id] = 0  # –ù–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                    
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö –ª–µ–π–±–ª–æ–≤ –¥–ª—è –ø–∞—Ä—ã {pair_id}: {e}")
            # –§–æ–ª–±—ç–∫ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –ª–µ–π–±–ª—ã
            for candidate in candidates:
                item_id = candidate['item_id']
                # –ü—Ä–æ—Å—Ç–æ–π –ª–µ–π–±–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑–æ–≤—ã—Ö scores
                simple_score = (
                    candidate.get('content_score', 0) * 0.4 +
                    candidate.get('cf_score', 0) * 0.3 +
                    candidate.get('embedding_score', 0) * 0.3
                )
                
                if simple_score >= 0.7:
                    labels[item_id] = 2
                elif simple_score >= 0.5:
                    labels[item_id] = 1
                else:
                    labels[item_id] = 0
        
        return labels
    
    def _get_base_labels_from_interactions(self, pair_id: str, candidates: List[Dict]) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –ª–µ–π–±–ª—ã –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"""
        labels = {}
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º enhanced interactions –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            interactions_data = self.enhanced_interactions if self.enhanced_interactions is not None else self.content_recommender.interactions
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–∞—Ä—ã
            if self.enhanced_pairs is not None:
                pair_info = self.enhanced_pairs[self.enhanced_pairs['id'] == pair_id]
            else:
                pair_info = self.content_recommender.pairs[self.content_recommender.pairs['id'] == pair_id]
            
            if pair_info.empty:
                return labels
            
            pair_row = pair_info.iloc[0]
            user1_id = pair_row['user1_id']
            user2_id = pair_row['user2_id']
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
            user_interactions = interactions_data[
                (interactions_data['user_id'] == user1_id) |
                (interactions_data['user_id'] == user2_id)
            ]
            
            # –°–æ–∑–¥–∞–µ–º –ª–µ–π–±–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
            for _, interaction in user_interactions.iterrows():
                product_id = interaction['product_id']
                rating = interaction.get('rating', 5)
                
                if pd.isna(rating):
                    continue
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–π—Ç–∏–Ω–≥ –≤ relevance score
                relevance = rating / 10.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                
                # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –µ—Å–ª–∏ –æ–±–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ—Ü–µ–Ω–∏–ª–∏
                if product_id in labels:
                    labels[product_id] = max(labels[product_id], relevance)
                else:
                    labels[product_id] = relevance
                    
        except Exception as e:
            pass
        
        return labels
    
    def _get_product_info(self, item_id: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–µ"""
        try:
            product = self.content_recommender.product_catalog[
                self.content_recommender.product_catalog['id'] == item_id
            ]
            if not product.empty:
                return product.iloc[0].to_dict()
        except:
            pass
        return {}
    
    def train_multi_objective_model(self, training_data: pd.DataFrame, groups: List[int], 
                                  validation_split: float = 0.2, n_folds: int = 3) -> Dict:
        """
        –û–±—É—á–∞–µ—Ç –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—É—é LightGBM –º–æ–¥–µ–ª—å
        
        Args:
            training_data: Training dataset with enhanced features
            groups: –†–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            n_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        print("üéØ –û–±—É—á–∞–µ–º Multi-Objective LightGBM –º–æ–¥–µ–ª—å...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        feature_cols = [col for col in training_data.columns 
                       if col not in ['pair_id', 'item_id', 'label']]
        self.feature_names = feature_cols
        
        X = training_data[feature_cols].values
        y = training_data['label'].values
        
        print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:")
        print(f"  - –§–∏—á–∏: {X.shape}")
        print(f"  - –õ–µ–π–±–ª—ã: {y.shape}")
        print(f"  - –ì—Ä—É–ø–ø—ã: {len(groups)}")
        print(f"  - –ù–æ–≤—ã—Ö –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö —Ñ–∏—á: {len([f for f in feature_cols if any(obj in f for obj in ['relevance', 'novelty', 'empathy'])])}")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        group_kfold = GroupKFold(n_splits=n_folds)
        cv_scores = []
        feature_importances = []
        
        pair_groups = training_data['pair_id'].values
        
        fold = 0
        for train_idx, val_idx in group_kfold.split(X, y, groups=pair_groups):
            fold += 1
            print(f"  üìä –§–æ–ª–¥ {fold}/{n_folds}")
            
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—ã –¥–ª—è train –∏ val
            train_groups = []
            val_groups = []
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—ã
            train_pair_count = len(set(training_data.iloc[train_idx]['pair_id']))
            val_pair_count = len(set(training_data.iloc[val_idx]['pair_id']))
            
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø
            avg_group_size = len(training_data) // len(groups)
            train_groups = [avg_group_size] * train_pair_count
            val_groups = [avg_group_size] * val_pair_count
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            if sum(train_groups) != len(X_train):
                train_groups[-1] += len(X_train) - sum(train_groups)
            if sum(val_groups) != len(X_val):
                val_groups[-1] += len(X_val) - sum(val_groups)
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)
            val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)
            
            model = lgb.train(
                self.lgb_params,
                train_data,
                valid_sets=[val_data],
                num_boost_round=1000,
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
            )
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            y_pred = model.predict(X_val)
            
            # –í—ã—á–∏—Å–ª—è–µ–º NDCG@10
            try:
                ndcg_scores = []
                start_idx = 0
                for group_size in val_groups:
                    end_idx = start_idx + group_size
                    if group_size > 1:
                        group_true = y_val[start_idx:end_idx]
                        group_pred = y_pred[start_idx:end_idx]
                        
                        ndcg = ndcg_score([group_true], [group_pred], k=min(10, group_size))
                        ndcg_scores.append(ndcg)
                    
                    start_idx = end_idx
                
                if ndcg_scores:
                    fold_ndcg = np.mean(ndcg_scores)
                    cv_scores.append(fold_ndcg)
                    print(f"    Multi-Objective NDCG@10: {fold_ndcg:.4f}")
                    
            except Exception as e:
                print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ NDCG: {e}")
            
            # Feature importance
            importance = model.feature_importance(importance_type='gain')
            feature_importances.append(importance)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        print("üéØ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ Multi-Objective –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
        train_data = lgb.Dataset(X, label=y, group=groups)
        
        self.multi_objective_model = lgb.train(
            self.lgb_params,
            train_data,
            num_boost_round=1000,
            callbacks=[lgb.log_evaluation(100)]
        )
        
        # Feature importance
        if feature_importances:
            avg_importance = np.mean(feature_importances, axis=0)
            self.feature_importance = dict(zip(self.feature_names, avg_importance))
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = {
            'cv_ndcg_mean': np.mean(cv_scores) if cv_scores else 0.0,
            'cv_ndcg_std': np.std(cv_scores) if cv_scores else 0.0,
            'n_folds': len(cv_scores),
            'feature_count': len(self.feature_names),
            'multi_objective_features': len([f for f in self.feature_names if any(obj in f for obj in ['relevance', 'novelty', 'empathy'])]),
            'training_samples': len(training_data),
            'training_groups': len(groups)
        }
        
        print(f"‚úÖ Multi-Objective –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        print(f"  CV NDCG@10: {metrics['cv_ndcg_mean']:.4f} ¬± {metrics['cv_ndcg_std']:.4f}")
        print(f"  –í—Å–µ–≥–æ —Ñ–∏—á: {metrics['feature_count']}")
        print(f"  –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö —Ñ–∏—á: {metrics['multi_objective_features']}")
        
        return metrics
    
    def rank_candidates_multi_objective(self, pair_id: str, candidates: List[Dict]) -> List[Dict]:
        """
        –†–∞–Ω–∂–∏—Ä—É–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            candidates: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            
        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
        """
        if self.multi_objective_model is None:
            print("‚ö†Ô∏è Multi-Objective –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return candidates
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º enhanced —Ñ–∏—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            features_list = []
            for candidate in candidates:
                features = self.extract_enhanced_features(pair_id, candidate['item_id'], candidate)
                feature_vector = [features.get(name, 0.0) for name in self.feature_names]
                features_list.append(feature_vector)
            
            if not features_list:
                return candidates
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º relevance scores
            X = np.array(features_list)
            scores = self.multi_objective_model.predict(X)
            
            # –î–æ–±–∞–≤–ª—è–µ–º scores –∏ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            enhanced_candidates = []
            for i, candidate in enumerate(candidates):
                # –í—ã—á–∏—Å–ª—è–µ–º –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏
                item_info = {
                    'title': candidate.get('title', ''),
                    'category': candidate.get('category', ''),
                    'price': candidate.get('price', 1000),
                    'tags': candidate.get('tags', []),
                    'love_language': candidate.get('love_language', 'quality_time'),
                    'novelty': candidate.get('novelty', 0.5)
                }
                
                multi_scores = self.calculate_multi_objective_score(pair_id, item_info, candidate)
                
                enhanced_candidate = candidate.copy()
                enhanced_candidate.update({
                    'multi_objective_score': scores[i],
                    'relevance_score': multi_scores.relevance,
                    'novelty_score': multi_scores.novelty,
                    'empathy_score': multi_scores.empathy,
                    'combined_score': multi_scores.combined,
                    'method': 'multi_objective_ranker'
                })
                
                enhanced_candidates.append(enhanced_candidate)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ multi-objective score
            ranked_candidates = sorted(enhanced_candidates, 
                                     key=lambda x: x['multi_objective_score'], reverse=True)
            
            return ranked_candidates
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–∞—Ä—ã {pair_id}: {e}")
            return candidates
    
    def save_multi_objective_model(self, model_path: str = 'models/multi_objective_v1'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å"""
        if self.multi_objective_model is None:
            print("‚ö†Ô∏è –ù–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º LightGBM –º–æ–¥–µ–ª—å
        self.multi_objective_model.save_model(f'{model_path}.txt')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_id': 'multi_objective_v1',
            'type': 'multi_objective_learning_to_rank',
            'version': '1.0',
            'algorithm': 'lightgbm_multi_objective_ranker',
            'feature_names': self.feature_names,
            'feature_importance': self.feature_importance,
            'objective_weights': self.objective_weights,
            'lgb_params': self.lgb_params,
            'objectives': ['relevance', 'novelty', 'empathy'],
            'created_at': datetime.now().isoformat()
        }
        
        with open(f'{model_path}_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Multi-Objective –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    def load_multi_objective_model(self, model_path: str = 'models/multi_objective_v1'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º LightGBM –º–æ–¥–µ–ª—å
            self.multi_objective_model = lgb.Booster(model_file=f'{model_path}.txt')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(f'{model_path}_metadata.json', 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            self.feature_names = metadata['feature_names']
            self.feature_importance = metadata['feature_importance']
            self.objective_weights = metadata['objective_weights']
            
            print(f"‚úÖ Multi-Objective –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å Multi-Objective –º–æ–¥–µ–ª—å: {e}")
            return False
    
    def get_feature_importance(self, top_k: int = 15) -> Dict[str, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö"""
        if not self.feature_importance:
            return {}
        
        sorted_features = sorted(self.feature_importance.items(), 
                               key=lambda x: x[1], reverse=True)
        
        return dict(sorted_features[:top_k])
    
    def analyze_multi_objective_performance(self, test_pairs: List[str] = None, num_test_pairs: int = 50) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            test_pairs: –°–ø–∏—Å–æ–∫ –ø–∞—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            num_test_pairs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        print("üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Multi-Objective Ranker...")
        
        if test_pairs is None:
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            if self.enhanced_pairs is not None:
                all_pairs = self.enhanced_pairs['id'].tolist()
            else:
                all_pairs = self.content_recommender.pairs['id'].tolist()
            
            test_pairs = np.random.choice(all_pairs, min(num_test_pairs, len(all_pairs)), replace=False)
        
        analysis_results = {
            'tested_pairs': len(test_pairs),
            'objective_scores': {'relevance': [], 'novelty': [], 'empathy': [], 'combined': []},
            'performance_metrics': {},
            'feature_analysis': {},
            'examples': []
        }
        
        for pair_id in test_pairs[:min(num_test_pairs, len(test_pairs))]:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                candidates = self._get_candidates_for_pair(pair_id, max_candidates=20)
                
                if len(candidates) < 3:
                    continue
                
                # –†–∞–Ω–∂–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏
                ranked_candidates = self.rank_candidates_multi_objective(pair_id, candidates)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                top_recommendations = ranked_candidates[:5]
                
                for rec in top_recommendations:
                    analysis_results['objective_scores']['relevance'].append(rec.get('relevance_score', 0))
                    analysis_results['objective_scores']['novelty'].append(rec.get('novelty_score', 0))
                    analysis_results['objective_scores']['empathy'].append(rec.get('empathy_score', 0))
                    analysis_results['objective_scores']['combined'].append(rec.get('combined_score', 0))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                if len(analysis_results['examples']) < 3:
                    example = {
                        'pair_id': pair_id,
                        'top_3_recommendations': [
                            {
                                'title': rec['title'],
                                'relevance': rec.get('relevance_score', 0),
                                'novelty': rec.get('novelty_score', 0),
                                'empathy': rec.get('empathy_score', 0),
                                'combined': rec.get('combined_score', 0),
                                'multi_objective_score': rec.get('multi_objective_score', 0)
                            }
                            for rec in top_recommendations[:3]
                        ]
                    }
                    analysis_results['examples'].append(example)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ä—ã {pair_id}: {e}")
                continue
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        for objective, scores in analysis_results['objective_scores'].items():
            if scores:
                analysis_results['performance_metrics'][objective] = {
                    'mean': round(np.mean(scores), 3),
                    'std': round(np.std(scores), 3),
                    'min': round(np.min(scores), 3),
                    'max': round(np.max(scores), 3)
                }
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á
        analysis_results['feature_analysis'] = {
            'top_features': self.get_feature_importance(10),
            'multi_objective_features': {
                name: importance for name, importance in self.feature_importance.items()
                if any(obj in name for obj in ['relevance', 'novelty', 'empathy'])
            }
        }
        
        return analysis_results

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Multi-Objective Ranker"""
    print("üéØ –ó–∞–ø—É—Å–∫ Multi-Objective Ranker - –§–∞–∑–∞ 2.1")
    print("üöÄ –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å + –ù–æ–≤–∏–∑–Ω–∞ + –≠–º–ø–∞—Ç–∏—è")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–Ω–∫–µ—Ä
    ranker = MultiObjectiveRanker()
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
    if not ranker.load_multi_objective_model():
        print("üîÑ –û–±—É—á–∞–µ–º –Ω–æ–≤—É—é Multi-Objective –º–æ–¥–µ–ª—å...")
        
        # –°–æ–∑–¥–∞–µ–º training dataset
        training_data, groups = ranker.create_multi_objective_training_dataset(sample_pairs=150)
        
        if len(training_data) == 0:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å training dataset")
            return
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        metrics = ranker.train_multi_objective_model(training_data, groups)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        ranker.save_multi_objective_model()
        
        print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è Multi-Objective –º–æ–¥–µ–ª–∏:")
        print(f"  CV NDCG@10: {metrics['cv_ndcg_mean']:.4f} ¬± {metrics['cv_ndcg_std']:.4f}")
        print(f"  –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö —Ñ–∏—á: {metrics['multi_objective_features']}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
    if ranker.enhanced_pairs is not None:
        test_pair_id = ranker.enhanced_pairs['id'].iloc[0]
    else:
        test_pair_id = ranker.content_recommender.pairs['id'].iloc[0]
    
    print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º Multi-Objective —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–∞—Ä—ã: {test_pair_id}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    candidates = ranker._get_candidates_for_pair(test_pair_id, max_candidates=10)
    
    if candidates:
        # –†–∞–Ω–∂–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        ranked_candidates = ranker.rank_candidates_multi_objective(test_pair_id, candidates)
        
        print(f"\nüìã –¢–æ–ø-5 Multi-Objective —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
        for i, candidate in enumerate(ranked_candidates[:5], 1):
            print(f"{i}. {candidate['title']}")
            print(f"   Multi-Objective Score: {candidate.get('multi_objective_score', 0):.4f}")
            print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {candidate.get('relevance_score', 0):.3f} | " +
                  f"–ù–æ–≤–∏–∑–Ω–∞: {candidate.get('novelty_score', 0):.3f} | " +
                  f"–≠–º–ø–∞—Ç–∏—è: {candidate.get('empathy_score', 0):.3f}")
            print(f"   –¶–µ–Ω–∞: {candidate['price']} —Ä—É–±. | –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {candidate['category']}")
            print()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    print("\nüìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã...")
    analysis = ranker.analyze_multi_objective_performance(num_test_pairs=20)
    
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Multi-Objective scores:")
    for objective, stats in analysis['performance_metrics'].items():
        print(f"  {objective.title()}: Œº={stats['mean']:.3f} œÉ={stats['std']:.3f} [{stats['min']:.3f}, {stats['max']:.3f}]")
    
    # –¢–æ–ø –≤–∞–∂–Ω—ã—Ö —Ñ–∏—á
    feature_importance = ranker.get_feature_importance(top_k=10)
    if feature_importance:
        print(f"\nüîç –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö —Ñ–∏—á Multi-Objective –º–æ–¥–µ–ª–∏:")
        for i, (feature, importance) in enumerate(feature_importance.items(), 1):
            print(f"{i:2d}. {feature}: {importance:.3f}")
    
    print(f"\nüéâ Multi-Objective Ranker –≥–æ—Ç–æ–≤!")
    print(f"‚úÖ –§–∞–∑–∞ 2.1 (–ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ) –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main()
