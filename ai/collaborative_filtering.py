import json
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import pickle
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class CollaborativeFilteringRecommender:
    def __init__(self, data_path: str = 'data/synthetic_v1'):
        self.data_path = data_path
        self.users = None
        self.pairs = None
        self.interactions = None
        self.product_catalog = None
        
        self.n_factors = 5
        self.learning_rate = 0.005
        self.regularization = 0.02
        self.n_epochs = 100
        
        self.svd_model = None
        self.user_item_matrix = None
        self.user_mapping = {}
        self.item_mapping = {}
        self.reverse_user_mapping = {}
        self.reverse_item_mapping = {}
        
        try:
            self.load_data()
        except Exception as e:
            logger.error(f"Failed to load data: {e}")
            raise
    
    def load_data(self):
        try:
            logger.info("Loading data for collaborative filtering")
            
            self.users = pd.read_csv(f'{self.data_path}/users.csv')
            self.pairs = pd.read_csv(f'{self.data_path}/pairs.csv')
            self.interactions = pd.read_csv(f'{self.data_path}/interactions.csv')
            self.product_catalog = pd.read_csv(f'{self.data_path}/product_catalog.csv')
            
            logger.info(f"Data loaded: {len(self.users)} users, {len(self.pairs)} pairs, {len(self.interactions)} interactions, {len(self.product_catalog)} products")
            
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise
    
    def prepare_user_item_matrix(self):
        try:
            logger.info("Preparing user-item matrix")
            
            if self.interactions is None or self.interactions.empty:
                logger.error("No interactions data available")
                return False
            
            ratings_data = self.interactions[
                (self.interactions['rating'].notna()) & 
                (self.interactions['rating'] > 0)
            ].copy()
            
            if len(ratings_data) == 0:
                logger.warning("No rating data available for training")
                return False
            
            unique_users = sorted(ratings_data['user_id'].unique())
            unique_items = sorted(ratings_data['product_id'].unique())
            
            self.user_mapping = {user_id: idx for idx, user_id in enumerate(unique_users)}
            self.item_mapping = {item_id: idx for idx, item_id in enumerate(unique_items)}
            
            self.reverse_user_mapping = {idx: user_id for user_id, idx in self.user_mapping.items()}
            self.reverse_item_mapping = {idx: item_id for item_id, idx in self.item_mapping.items()}
            
            n_users = len(unique_users)
            n_items = len(unique_items)
            
            self.user_item_matrix = np.zeros((n_users, n_items))
            
            for _, row in ratings_data.iterrows():
                user_idx = self.user_mapping[row['user_id']]
                item_idx = self.item_mapping[row['product_id']]
                rating = row['rating']
                
                self.user_item_matrix[user_idx, item_idx] = rating
            
            fill_rate = (self.user_item_matrix > 0).sum() / (n_users * n_items) * 100
            logger.info(f"Matrix created: {n_users} users √ó {n_items} items, fill rate: {fill_rate:.2f}%")
            
            return True
            
        except Exception as e:
            logger.error(f"Error preparing user-item matrix: {e}")
            return False
    
    def train_svd_model(self):
        if self.user_item_matrix is None:
            if not self.prepare_user_item_matrix():
                return False
        
        try:
            logger.info(f"Training SVD model (n_factors={self.n_factors})")
            
            self.svd_model = TruncatedSVD(
                n_components=self.n_factors,
                random_state=42
            )
            
            self.svd_model.fit(self.user_item_matrix)
            self.user_item_matrix_transformed = self.svd_model.transform(self.user_item_matrix)
            
            logger.info("SVD model trained successfully")
            return True
            
        except Exception as e:
            logger.error(f"Error training SVD model: {e}")
            return False
    
    def predict_rating(self, user_id: str, item_id: str) -> float:
        try:
            if self.svd_model is None:
                return 0.0
            
            if user_id not in self.user_mapping or item_id not in self.item_mapping:
                return 0.0
            
            user_idx = self.user_mapping[user_id]
            item_idx = self.item_mapping[item_id]
            
            user_factors = self.svd_model.transform(self.user_item_matrix[user_idx:user_idx+1])[0]
            item_factors = self.svd_model.components_[:, item_idx]
            
            predicted_rating = np.dot(user_factors, item_factors)
            
            return max(1.0, min(10.0, predicted_rating))
            
        except Exception as e:
            logger.warning(f"Error predicting rating for user {user_id}, item {item_id}: {e}")
            return 0.0
    
    def get_user_recommendations(self, user_id: str, top_k: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if self.svd_model is None:
            return []
        
        if user_id not in self.user_mapping:
            return []
        
        user_idx = self.user_mapping[user_id]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_factors = self.svd_model.transform(self.user_item_matrix[user_idx:user_idx+1])[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
        predictions = []
        for item_id, item_idx in self.item_mapping.items():
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–≤–∞—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∂–µ –æ—Ü–µ–Ω–∏–ª
            if self.user_item_matrix[user_idx, item_idx] > 0:
                continue
            
            item_factors = self.svd_model.components_[:, item_idx]
            predicted_rating = np.dot(user_factors, item_factors)
            
            predictions.append({
                'item_id': item_id,
                'predicted_rating': max(1.0, min(10.0, predicted_rating))
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º—É —Ä–µ–π—Ç–∏–Ω–≥—É
        predictions.sort(key=lambda x: x['predicted_rating'], reverse=True)
        
        return predictions[:top_k]
    
    def get_pair_recommendations(self, pair_id: str, top_k: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—ã"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–∞—Ä—ã
            pair = self.pairs[self.pairs['id'] == pair_id]
            if pair.empty:
                return []
            
            pair = pair.iloc[0]
            user1_id = pair['user1_id']
            user2_id = pair['user2_id']
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user1_recs = self.get_user_recommendations(user1_id, top_k * 2)
            user2_recs = self.get_user_recommendations(user2_id, top_k * 2)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏
            combined_recs = {}
            
            for rec in user1_recs:
                item_id = rec['item_id']
                combined_recs[item_id] = {
                    'item_id': item_id,
                    'user1_rating': rec['predicted_rating'],
                    'user2_rating': 0.0,
                    'combined_rating': rec['predicted_rating']
                }
            
            for rec in user2_recs:
                item_id = rec['item_id']
                if item_id in combined_recs:
                    combined_recs[item_id]['user2_rating'] = rec['predicted_rating']
                    combined_recs[item_id]['combined_rating'] = (
                        combined_recs[item_id]['user1_rating'] + rec['predicted_rating']
                    ) / 2
                else:
                    combined_recs[item_id] = {
                        'item_id': item_id,
                        'user1_rating': 0.0,
                        'user2_rating': rec['predicted_rating'],
                        'combined_rating': rec['predicted_rating']
                    }
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ä–µ–π—Ç–∏–Ω–≥—É
            final_recs = list(combined_recs.values())
            final_recs.sort(key=lambda x: x['combined_rating'], reverse=True)
            
            return final_recs[:top_k]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–∞—Ä—ã: {e}")
            return []
    
    def evaluate_model(self, test_size: float = 0.2) -> Dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        if self.user_item_matrix is None:
            return {'error': 'Model not trained'}
        
        print("üìä –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É
        ratings_data = self.interactions[
            (self.interactions['rating'].notna()) & 
            (self.interactions['rating'] > 0)
        ].copy()
        
        if len(ratings_data) < 100:
            return {'error': 'Not enough data for evaluation'}
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
        train_data, test_data = train_test_split(
            ratings_data, 
            test_size=test_size, 
            random_state=42
        )
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
        train_matrix = np.zeros_like(self.user_item_matrix)
        
        for _, row in train_data.iterrows():
            if row['user_id'] in self.user_mapping and row['product_id'] in self.item_mapping:
                user_idx = self.user_mapping[row['user_id']]
                item_idx = self.item_mapping[row['product_id']]
                train_matrix[user_idx, item_idx] = row['rating']
        
        # –û–±—É—á–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        temp_model = TruncatedSVD(
            n_components=self.n_factors,
            random_state=42
        )
        temp_model.fit(train_matrix)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞ test –¥–∞–Ω–Ω—ã—Ö
        predictions = []
        actual_ratings = []
        
        for _, row in test_data.iterrows():
            if row['user_id'] in self.user_mapping and row['product_id'] in self.item_mapping:
                user_idx = self.user_mapping[row['user_id']]
                item_idx = self.item_mapping[row['product_id']]
                
                user_factors = temp_model.transform(train_matrix[user_idx:user_idx+1])[0]
                item_factors = temp_model.components_[:, item_idx]
                predicted_rating = np.dot(user_factors, item_factors)
                
                predictions.append(max(1.0, min(10.0, predicted_rating)))
                actual_ratings.append(row['rating'])
        
        if len(predictions) == 0:
            return {'error': 'No valid predictions'}
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        rmse = np.sqrt(mean_squared_error(actual_ratings, predictions))
        mae = np.mean(np.abs(np.array(actual_ratings) - np.array(predictions)))
        
        # –í—ã—á–∏—Å–ª—è–µ–º NDCG@10 –ü–†–ê–í–ò–õ–¨–ù–û - –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º!
        print("üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –ü–†–ê–í–ò–õ–¨–ù–£–Æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é NDCG (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º)...")
        
        ndcg_scores = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        test_user_groups = test_data.groupby('user_id')
        users_processed = 0
        
        for user_id, user_group in test_user_groups:
            if user_id not in self.user_mapping or len(user_group) < 2:
                continue  # NDCG —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —ç–ª–µ–º–µ–Ω—Ç–∞
            
            users_processed += 1
            user_idx = self.user_mapping[user_id]
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_item_ids = user_group['product_id'].values
            user_item_indices = []
            user_actual_ratings = []
            
            for i, item_id in enumerate(user_item_ids):
                if item_id in self.item_mapping:
                    user_item_indices.append(self.item_mapping[item_id])
                    user_actual_ratings.append(user_group.iloc[i]['rating'])
            
            if len(user_item_indices) < 2:
                continue
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏ –¢–û–õ–¨–ö–û –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤ —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            # –ü–æ–ª—É—á–∞–µ–º user factors –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_factors = self.user_item_matrix_transformed[user_idx]
            
            user_predictions = []
            for item_idx in user_item_indices:
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ = user_factors @ item_factors 
                item_factors = self.svd_model.components_[:, item_idx]
                prediction = np.dot(user_factors, item_factors)
                user_predictions.append(prediction)
            
            user_predictions = np.array(user_predictions)
            user_actual_ratings = np.array(user_actual_ratings)
            
            # –°—á–∏—Ç–∞–µ–º NDCG –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if len(user_predictions) >= 2:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º (—Ç–æ–ø —Ç–æ–≤–∞—Ä—ã –¥–ª—è –≠–¢–û–ì–û –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
                sorted_indices = np.argsort(user_predictions)[::-1]
                
                # DCG –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                dcg = sum(user_actual_ratings[idx] / np.log2(i + 2) 
                         for i, idx in enumerate(sorted_indices[:10]))
                
                # IDCG –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–¥–µ–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ï–ì–û —Ä–µ–π—Ç–∏–Ω–≥–æ–≤)
                sorted_actual = sorted(user_actual_ratings, reverse=True)
                idcg = sum(rating / np.log2(i + 2) 
                          for i, rating in enumerate(sorted_actual[:10]))
                
                if idcg > 0:
                    user_ndcg = dcg / idcg
                    ndcg_scores.append(user_ndcg)
        
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è NDCG: {users_processed}")
        print(f"üìä –í–∞–ª–∏–¥–Ω—ã—Ö NDCG —Ä–∞—Å—á–µ—Ç–æ–≤: {len(ndcg_scores)}")
        
        metrics = {
            'rmse': float(rmse),
            'mae': float(mae),
            'ndcg_at_10': float(np.mean(ndcg_scores)) if ndcg_scores else 0.0,
            'num_predictions': len(predictions),
            'test_size': len(test_data),
            'train_size': len(train_data)
        }
        
        print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã:")
        print(f"  RMSE: {metrics['rmse']:.3f}")
        print(f"  MAE: {metrics['mae']:.3f}")
        print(f"  NDCG@10: {metrics['ndcg_at_10']:.3f}")
        
        return metrics
    
    def save_model(self, model_path: str = 'models/cf_svd_v1'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        import os
        os.makedirs('models', exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        with open(f'{model_path}.pkl', 'wb') as f:
            pickle.dump({
                'svd_model': self.svd_model,
                'user_mapping': self.user_mapping,
                'item_mapping': self.item_mapping,
                'reverse_user_mapping': self.reverse_user_mapping,
                'reverse_item_mapping': self.reverse_item_mapping,
                'n_factors': self.n_factors
            }, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_id': 'cf_svd_v1',
            'type': 'collaborative_filtering',
            'version': '1.0',
            'trained_on': {
                'num_users': len(self.user_mapping),
                'num_items': len(self.item_mapping),
                'num_interactions': len(self.interactions)
            },
            'parameters': {
                'n_factors': self.n_factors,
                'learning_rate': self.learning_rate,
                'regularization': self.regularization,
                'n_epochs': self.n_epochs
            },
            'created_at': datetime.now().isoformat()
        }
        
        with open(f'{model_path}_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    def load_model(self, model_path: str = 'models/cf_svd_v1'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        try:
            with open(f'{model_path}.pkl', 'rb') as f:
                model_data = pickle.load(f)
            
            self.svd_model = model_data['svd_model']
            self.user_mapping = model_data['user_mapping']
            self.item_mapping = model_data['item_mapping']
            self.reverse_user_mapping = model_data['reverse_user_mapping']
            self.reverse_item_mapping = model_data['reverse_item_mapping']
            self.n_factors = model_data['n_factors']
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Collaborative Filtering Recommendation System")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É
    cf_recommender = CollaborativeFilteringRecommender()
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    if not cf_recommender.train_svd_model():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å")
        return
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
    metrics = cf_recommender.evaluate_model()
    if 'error' not in metrics:
        print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
        print(f"  RMSE: {metrics['rmse']:.3f}")
        print(f"  MAE: {metrics['mae']:.3f}")
        print(f"  NDCG@10: {metrics['ndcg_at_10']:.3f}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    test_pair_id = cf_recommender.pairs['id'].iloc[0]
    print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—ã: {test_pair_id}")
    
    recommendations = cf_recommender.get_pair_recommendations(test_pair_id, top_k=5)
    
    print(f"\nüìã –¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π CF:")
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. Item ID: {rec['item_id']}")
        print(f"   Combined Rating: {rec['combined_rating']:.3f}")
        print(f"   User1 Rating: {rec['user1_rating']:.3f}")
        print(f"   User2 Rating: {rec['user2_rating']:.3f}")
        print()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    cf_recommender.save_model()
    
    print("üéâ Collaborative Filtering —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞!")

if __name__ == "__main__":
    main()
