import json
import os
import pickle
import time
import logging
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class EmbeddingService:
    def __init__(self, data_path: str = 'data/synthetic_v1', model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):
        self.data_path = data_path
        self.model_name = model_name
        
        try:
            logger.info(f"Loading model {model_name}")
            self.embedding_model = SentenceTransformer(model_name)
            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(f"Model loaded, embedding dimension: {self.embedding_dim}")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            raise
        
        self.users = None
        self.pairs = None
        self.product_catalog = None
        self.interactions = None
        
        self.user_embeddings = {}
        self.product_embeddings = {}
        self.pair_embeddings = {}
        
        self.user_index = None
        self.product_index = None
        self.user_id_mapping = {}
        self.product_id_mapping = {}
        
        os.makedirs('embeddings_store', exist_ok=True)
        
        try:
            self.load_data()
        except Exception as e:
            logger.error(f"Failed to load data: {e}")
            raise
    
    def load_data(self):
        try:
            logger.info("Loading data")
            self.users = pd.read_csv(f'{self.data_path}/users.csv')
            self.pairs = pd.read_csv(f'{self.data_path}/pairs.csv')
            self.product_catalog = pd.read_csv(f'{self.data_path}/product_catalog.csv')
            self.interactions = pd.read_csv(f'{self.data_path}/interactions.csv')
            
            logger.info(f"Data loaded: {len(self.users)} users, {len(self.pairs)} pairs, {len(self.product_catalog)} products, {len(self.interactions)} interactions")
            
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise
    
    def create_user_text_representation(self, user_row: pd.Series) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        
        Args:
            user_row: –°—Ç—Ä–æ–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ DataFrame
        
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        try:
            # –ü–∞—Ä—Å–∏–º –∏–Ω—Ç–µ—Ä–µ—Å—ã
            interests_dict = eval(user_row['interests'])
            top_interests = sorted(interests_dict.items(), key=lambda x: x[1], reverse=True)[:10]
            interests_text = ', '.join([f"{interest}" for interest, score in top_interests])
            
            # –ü–∞—Ä—Å–∏–º —è–∑—ã–∫–∏ –ª—é–±–≤–∏
            love_languages_dict = eval(user_row['love_languages'])
            primary_love_language = max(love_languages_dict.items(), key=lambda x: x[1])[0]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            user_text = f"""
            –í–æ–∑—Ä–∞—Å—Ç: {user_row['age']}
            –ü–æ–ª: {user_row['gender']}
            –ì–æ—Ä–æ–¥: {user_row['city']}
            –ê—Ä—Ö–µ—Ç–∏–ø: {user_row['archetype']}
            –ò–Ω—Ç–µ—Ä–µ—Å—ã: {interests_text}
            –û—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ –ª—é–±–≤–∏: {primary_love_language}
            –ë—é–¥–∂–µ—Ç–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è: {user_row['budget_preference']}
            """.strip()
            
            return user_text
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {e}")
            return f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_row['age']} –ª–µ—Ç, {user_row['gender']}, {user_row['city']}"
    
    def create_product_text_representation(self, product_row: pd.Series) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        
        Args:
            product_row: –°—Ç—Ä–æ–∫–∞ —Ç–æ–≤–∞—Ä–∞ –∏–∑ DataFrame
        
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
        """
        try:
            # –ü–∞—Ä—Å–∏–º —Ç–µ–≥–∏
            tags_list = eval(product_row['tags'])
            tags_text = ', '.join(tags_list)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            product_text = f"""
            –ù–∞–∑–≤–∞–Ω–∏–µ: {product_row['title']}
            –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {product_row['category']}
            –¢–µ–≥–∏: {tags_text}
            –Ø–∑—ã–∫ –ª—é–±–≤–∏: {product_row['love_language']}
            –¶–µ–Ω–∞: {product_row['price']} —Ä—É–±–ª–µ–π
            """.strip()
            
            return product_text
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–∞: {e}")
            return f"{product_row['title']} - {product_row['category']}"
    
    def create_pair_text_representation(self, pair_id: str) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
        
        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–∞—Ä—ã
        """
        try:
            pair = self.pairs[self.pairs['id'] == pair_id].iloc[0]
            user1 = self.users[self.users['id'] == pair['user1_id']].iloc[0]
            user2 = self.users[self.users['id'] == pair['user2_id']].iloc[0]
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –ø–∞—Ä—ã
            user1_text = self.create_user_text_representation(user1)
            user2_text = self.create_user_text_representation(user2)
            
            pair_text = f"–ü–∞—Ä–∞:\n–ü–∞—Ä—Ç–Ω–µ—Ä 1: {user1_text}\n–ü–∞—Ä—Ç–Ω–µ—Ä 2: {user2_text}"
            
            return pair_text
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–∞—Ä—ã {pair_id}: {e}")
            return f"–ü–∞—Ä–∞ {pair_id}"
    
    def generate_user_embeddings(self, batch_size: int = 32) -> Dict[str, np.ndarray]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {user_id: embedding}
        """
        print("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        user_texts = []
        user_ids = []
        
        for idx, user_row in self.users.iterrows():
            user_text = self.create_user_text_representation(user_row)
            user_texts.append(user_text)
            user_ids.append(user_row['id'])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
        embeddings = []
        for i in range(0, len(user_texts), batch_size):
            batch_texts = user_texts[i:i + batch_size]
            batch_embeddings = self.embedding_model.encode(batch_texts, show_progress_bar=False)
            embeddings.extend(batch_embeddings)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        self.user_embeddings = {user_id: emb for user_id, emb in zip(user_ids, embeddings)}
        
        duration = time.time() - start_time
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {len(self.user_embeddings)} –∑–∞ {duration:.2f}—Å")
        
        return self.user_embeddings
    
    def generate_product_embeddings(self, batch_size: int = 32) -> Dict[str, np.ndarray]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {product_id: embedding}
        """
        print("üõçÔ∏è –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤...")
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        product_texts = []
        product_ids = []
        
        for idx, product_row in self.product_catalog.iterrows():
            product_text = self.create_product_text_representation(product_row)
            product_texts.append(product_text)
            product_ids.append(product_row['id'])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
        embeddings = []
        for i in range(0, len(product_texts), batch_size):
            batch_texts = product_texts[i:i + batch_size]
            batch_embeddings = self.embedding_model.encode(batch_texts, show_progress_bar=False)
            embeddings.extend(batch_embeddings)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        self.product_embeddings = {product_id: emb for product_id, emb in zip(product_ids, embeddings)}
        
        duration = time.time() - start_time
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {len(self.product_embeddings)} –∑–∞ {duration:.2f}—Å")
        
        return self.product_embeddings
    
    def generate_pair_embeddings(self, batch_size: int = 32) -> Dict[str, np.ndarray]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {pair_id: embedding}
        """
        print("üíï –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–∞—Ä...")
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        pair_texts = []
        pair_ids = []
        
        for idx, pair_row in self.pairs.iterrows():
            pair_text = self.create_pair_text_representation(pair_row['id'])
            pair_texts.append(pair_text)
            pair_ids.append(pair_row['id'])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
        embeddings = []
        for i in range(0, len(pair_texts), batch_size):
            batch_texts = pair_texts[i:i + batch_size]
            batch_embeddings = self.embedding_model.encode(batch_texts, show_progress_bar=False)
            embeddings.extend(batch_embeddings)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        self.pair_embeddings = {pair_id: emb for pair_id, emb in zip(pair_ids, embeddings)}
        
        duration = time.time() - start_time
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–∞—Ä —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {len(self.pair_embeddings)} –∑–∞ {duration:.2f}—Å")
        
        return self.pair_embeddings
    
    def build_faiss_indexes(self):
        """–°—Ç—Ä–æ–∏—Ç Faiss –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        print("üöÄ –°—Ç—Ä–æ–∏–º Faiss –∏–Ω–¥–µ–∫—Å—ã...")
        
        # –ò–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        if self.user_embeddings:
            user_embeddings_matrix = np.array(list(self.user_embeddings.values())).astype('float32')
            self.user_index = faiss.IndexFlatL2(self.embedding_dim)
            self.user_index.add(user_embeddings_matrix)
            
            # –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            self.user_id_mapping = {i: user_id for i, user_id in enumerate(self.user_embeddings.keys())}
            print(f"‚úÖ User index: {self.user_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
        
        # –ò–Ω–¥–µ–∫—Å –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤
        if self.product_embeddings:
            product_embeddings_matrix = np.array(list(self.product_embeddings.values())).astype('float32')
            self.product_index = faiss.IndexFlatL2(self.embedding_dim)
            self.product_index.add(product_embeddings_matrix)
            
            # –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            self.product_id_mapping = {i: product_id for i, product_id in enumerate(self.product_embeddings.keys())}
            print(f"‚úÖ Product index: {self.product_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
    
    def find_similar_products_ann(self, pair_id: str, top_k: int = 10) -> List[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã —Å –ø–æ–º–æ—â—å—é ANN –ø–æ–∏—Å–∫–∞
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å cosine similarity
        """
        try:
            if pair_id not in self.pair_embeddings:
                print(f"‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –ø–∞—Ä—ã {pair_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return []
            
            if self.product_index is None:
                print("‚ö†Ô∏è Product index –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
                return []
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–∞—Ä—ã
            pair_embedding = self.pair_embeddings[pair_id].astype('float32').reshape(1, -1)
            
            # –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É
            distances, indices = self.product_index.search(pair_embedding, top_k)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ cosine similarity –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            candidates = []
            for distance, idx in zip(distances[0], indices[0]):
                if idx == -1:  # Faiss –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
                    continue
                
                product_id = self.product_id_mapping[idx]
                cosine_similarity = 1.0 / (1.0 + distance)  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Å–∏—è
                
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ
                product_info = self.product_catalog[self.product_catalog['id'] == product_id].iloc[0]
                
                candidates.append({
                    'item_id': product_id,
                    'title': product_info['title'],
                    'category': product_info['category'],
                    'price': product_info['price'],
                    'embedding_similarity': cosine_similarity,
                    'search_method': 'ann_embedding'
                })
            
            return candidates
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ ANN –ø–æ–∏—Å–∫–∞ –¥–ª—è –ø–∞—Ä—ã {pair_id}: {e}")
            return []
    
    def save_embeddings(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –¥–∏—Å–∫"""
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        with open('embeddings_store/user_embeddings.pkl', 'wb') as f:
            pickle.dump(self.user_embeddings, f)
        
        with open('embeddings_store/product_embeddings.pkl', 'wb') as f:
            pickle.dump(self.product_embeddings, f)
        
        with open('embeddings_store/pair_embeddings.pkl', 'wb') as f:
            pickle.dump(self.pair_embeddings, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Faiss –∏–Ω–¥–µ–∫—Å—ã
        if self.user_index:
            faiss.write_index(self.user_index, 'embeddings_store/user_index.faiss')
        
        if self.product_index:
            faiss.write_index(self.product_index, 'embeddings_store/product_index.faiss')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
        with open('embeddings_store/user_id_mapping.json', 'w') as f:
            json.dump(self.user_id_mapping, f)
        
        with open('embeddings_store/product_id_mapping.json', 'w') as f:
            json.dump(self.product_id_mapping, f)
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_name': self.model_name,
            'embedding_dim': self.embedding_dim,
            'num_users': len(self.user_embeddings),
            'num_products': len(self.product_embeddings),
            'num_pairs': len(self.pair_embeddings),
            'created_at': datetime.now().isoformat()
        }
        
        with open('embeddings_store/metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ embeddings_store/")
    
    def load_embeddings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∏–Ω–¥–µ–∫—Å—ã —Å –¥–∏—Å–∫–∞"""
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            with open('embeddings_store/user_embeddings.pkl', 'rb') as f:
                self.user_embeddings = pickle.load(f)
            
            with open('embeddings_store/product_embeddings.pkl', 'rb') as f:
                self.product_embeddings = pickle.load(f)
            
            with open('embeddings_store/pair_embeddings.pkl', 'rb') as f:
                self.pair_embeddings = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º Faiss –∏–Ω–¥–µ–∫—Å—ã
            if os.path.exists('embeddings_store/user_index.faiss'):
                self.user_index = faiss.read_index('embeddings_store/user_index.faiss')
            
            if os.path.exists('embeddings_store/product_index.faiss'):
                self.product_index = faiss.read_index('embeddings_store/product_index.faiss')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
            with open('embeddings_store/user_id_mapping.json', 'r') as f:
                user_mapping_str = json.load(f)
                self.user_id_mapping = {int(k): v for k, v in user_mapping_str.items()}
            
            with open('embeddings_store/product_id_mapping.json', 'r') as f:
                product_mapping_str = json.load(f)
                self.product_id_mapping = {int(k): v for k, v in product_mapping_str.items()}
            
            print("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: {e}")
            return False
    
    def benchmark_search_latency(self, num_queries: int = 100) -> Dict[str, float]:
        """
        –ë–µ–Ω—á–º–∞—Ä–∫ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞
        
        Args:
            num_queries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        """
        print(f"‚è±Ô∏è –ë–µ–Ω—á–º–∞—Ä–∫ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ {num_queries} –∑–∞–ø—Ä–æ—Å–∞—Ö...")
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_pairs = np.random.choice(list(self.pair_embeddings.keys()), 
                                     min(num_queries, len(self.pair_embeddings)), 
                                     replace=False)
        
        latencies = []
        
        for pair_id in test_pairs:
            start_time = time.time()
            candidates = self.find_similar_products_ann(pair_id, top_k=10)
            latency = (time.time() - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
            latencies.append(latency)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = {
            'mean_latency_ms': np.mean(latencies),
            'p50_latency_ms': np.percentile(latencies, 50),
            'p95_latency_ms': np.percentile(latencies, 95),
            'p99_latency_ms': np.percentile(latencies, 99),
            'max_latency_ms': np.max(latencies),
            'num_queries': len(latencies)
        }
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞:")
        print(f"  Mean latency: {stats['mean_latency_ms']:.2f}ms")
        print(f"  P95 latency: {stats['p95_latency_ms']:.2f}ms")
        print(f"  P99 latency: {stats['p99_latency_ms']:.2f}ms")
        
        return stats

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Embedding Service"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Embedding Service –¥–ª—è LoveMemory AI")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å
    embedding_service = EmbeddingService()
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    if not embedding_service.load_embeddings():
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ
        print("üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        embedding_service.generate_user_embeddings()
        embedding_service.generate_product_embeddings()
        embedding_service.generate_pair_embeddings()
        
        # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å—ã
        embedding_service.build_faiss_indexes()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
        embedding_service.save_embeddings()
    else:
        print("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –¥–∏—Å–∫–∞")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    test_pair_id = list(embedding_service.pair_embeddings.keys())[0]
    print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º ANN –ø–æ–∏—Å–∫ –¥–ª—è –ø–∞—Ä—ã: {test_pair_id}")
    
    candidates = embedding_service.find_similar_products_ann(test_pair_id, top_k=5)
    
    print(f"\nüìã –¢–æ–ø-5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º:")
    for i, candidate in enumerate(candidates, 1):
        print(f"{i}. {candidate['title']}")
        print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {candidate['category']}")
        print(f"   Embedding Similarity: {candidate['embedding_similarity']:.3f}")
        print(f"   –¶–µ–Ω–∞: {candidate['price']} —Ä—É–±.")
        print()
    
    # –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    stats = embedding_service.benchmark_search_latency(num_queries=50)
    
    if stats['p95_latency_ms'] > 300:
        print("‚ö†Ô∏è P95 latency > 300ms, —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
    else:
        print("‚úÖ –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã")
    
    print("\nüéâ Embedding Service –≥–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏!")

if __name__ == "__main__":
    main()
