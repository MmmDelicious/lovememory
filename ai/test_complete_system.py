#!/usr/bin/env python3
"""
Comprehensive Test Suite –¥–ª—è LoveMemory AI
–ü–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö 9 —Ñ–∞–∑ AI —Å–∏—Å—Ç–µ–º—ã

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç:
- –§–∞–∑—ã 1-3: –î–∞–Ω–Ω—ã–µ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –§–∞–∑–∞ 4: Content-Based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –§–∞–∑–∞ 5: Collaborative Filtering
- –§–∞–∑–∞ 6: Embeddings + ANN –ø–æ–∏—Å–∫
- –§–∞–∑–∞ 7: Learning to Rank
- –§–∞–∑–∞ 8: LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—è + Explainability
- –§–∞–∑–∞ 9: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ + ETL
"""

import time
import json
import os
import sys
from datetime import datetime
import pandas as pd
import numpy as np
from typing import Dict, List, Any

# –ò–º–ø–æ—Ä—Ç—ã –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from content_recommender import ContentBasedRecommender
from collaborative_filtering import CollaborativeFilteringRecommender
from embedding_service import EmbeddingService
from learning_to_rank_service import LearningToRankService
from llm_wrapper import LLMWrapper
from explainability_service import ExplainabilityService
from ultimate_ai_service import UltimateAIService
from monitoring_service import MonitoringService

# ETL
sys.path.append('etl')
from prepare_train import ETLPipeline

class ComprehensiveTestSuite:
    """Comprehensive —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –≤—Å–µ–π AI —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞"""
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'phases_tested': [],
            'passed_tests': [],
            'failed_tests': [],
            'performance_metrics': {},
            'recommendations_samples': [],
            'errors': []
        }
        
        print("üß™ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Comprehensive Test Suite")
        print("="*80)
    
    def run_all_tests(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã"""
        start_time = time.time()
        
        print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø AI –°–ò–°–¢–ï–ú–´")
        print("üìÖ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        print("="*80)
        
        # –§–∞–∑—ã 1-3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        print("\nüîµ –§–ê–ó–´ 1-3: –î–ê–ù–ù–´–ï –ò –ê–†–•–ò–¢–ï–ö–¢–£–†–ê")
        self._test_data_architecture()
        
        # –§–∞–∑–∞ 4: Content-Based
        print("\nüü¢ –§–ê–ó–ê 4: CONTENT-BASED –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        self._test_content_based()
        
        # –§–∞–∑–∞ 5: Collaborative Filtering
        print("\nüü° –§–ê–ó–ê 5: COLLABORATIVE FILTERING")
        self._test_collaborative_filtering()
        
        # –§–∞–∑–∞ 6: Embeddings
        print("\nüü£ –§–ê–ó–ê 6: EMBEDDINGS + ANN –ü–û–ò–°–ö")
        self._test_embeddings()
        
        # –§–∞–∑–∞ 7: Learning to Rank
        print("\nüî¥ –§–ê–ó–ê 7: LEARNING TO RANK")
        self._test_learning_to_rank()
        
        # –§–∞–∑–∞ 8: LLM + Explainability
        print("\nüü† –§–ê–ó–ê 8: LLM + EXPLAINABILITY")
        self._test_llm_explainability()
        
        # –§–∞–∑–∞ 9: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ + ETL
        print("\n‚ö™ –§–ê–ó–ê 9: –ú–û–ù–ò–¢–û–†–ò–ù–ì + ETL")
        self._test_monitoring_etl()
        
        # Ultimate Integration Test
        print("\nüåü ULTIMATE INTEGRATION TEST")
        self._test_ultimate_integration()
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        print("\n‚ö° –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
        self._test_performance()
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        total_time = time.time() - start_time
        self.results['total_test_time_minutes'] = total_time / 60
        
        self._generate_final_report()
        
        return self.results
    
    def _test_data_architecture(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö (–§–∞–∑—ã 1-3)"""
        phase_name = "data_architecture"
        self.results['phases_tested'].append(phase_name)
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
            data_files = [
                'data/synthetic_v1/users.csv',
                'data/synthetic_v1/pairs.csv', 
                'data/synthetic_v1/product_catalog.csv',
                'data/synthetic_v1/interactions.csv'
            ]
            
            for file_path in data_files:
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: {file_path}")
                
                df = pd.read_csv(file_path)
                if df.empty:
                    raise ValueError(f"–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path}")
                
                print(f"  ‚úÖ {file_path}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            users = pd.read_csv('data/synthetic_v1/users.csv')
            pairs = pd.read_csv('data/synthetic_v1/pairs.csv')
            interactions = pd.read_csv('data/synthetic_v1/interactions.csv')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–∏
            user_ids = set(users['id'])
            pair_users = set(pairs['user1_id']) | set(pairs['user2_id'])
            interaction_users = set(interactions['user_id'])
            
            if not pair_users.issubset(user_ids):
                raise ValueError("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –ø–∞—Ä–∞—Ö")
            
            if not interaction_users.issubset(user_ids):
                raise ValueError("–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö")
            
            self.results['performance_metrics']['users_count'] = len(users)
            self.results['performance_metrics']['pairs_count'] = len(pairs)
            self.results['performance_metrics']['interactions_count'] = len(interactions)
            
            self.results['passed_tests'].append(f"{phase_name}_data_integrity")
            print("  ‚úÖ –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_content_based(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç Content-Based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–§–∞–∑–∞ 4)"""
        phase_name = "content_based"
        self.results['phases_tested'].append(phase_name)
        
        try:
            start_time = time.time()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
            content_rec = ContentBasedRecommender()
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            test_pair_id = content_rec.pairs['id'].iloc[0]
            recommendations = content_rec.recommend_date(test_pair_id, top_k=5)
            
            if not recommendations:
                raise ValueError("–ù–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç Content-Based —Å–∏—Å—Ç–µ–º—ã")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            for rec in recommendations:
                if not hasattr(rec, 'score') or rec.score < 0:
                    raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ scores –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö")
            
            processing_time = time.time() - start_time
            
            self.results['performance_metrics']['content_based_latency_ms'] = processing_time * 1000
            self.results['performance_metrics']['content_based_recommendations'] = len(recommendations)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã
            sample_recs = [
                {
                    'title': rec.title,
                    'score': rec.score,
                    'category': rec.category,
                    'phase': 'content_based'
                }
                for rec in recommendations[:3]
            ]
            self.results['recommendations_samples'].extend(sample_recs)
            
            self.results['passed_tests'].append(f"{phase_name}_generation")
            self.results['passed_tests'].append(f"{phase_name}_performance")
            print(f"  ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {len(recommendations)}, –í—Ä–µ–º—è: {processing_time*1000:.2f}ms")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_collaborative_filtering(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç Collaborative Filtering (–§–∞–∑–∞ 5)"""
        phase_name = "collaborative_filtering"
        self.results['phases_tested'].append(phase_name)
        
        try:
            start_time = time.time()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º CF —Å–∏—Å—Ç–µ–º—É
            cf_rec = CollaborativeFilteringRecommender()
            
            # –û–±—É—á–∞–µ–º/–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            if not cf_rec.train_svd_model():
                print("  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å CF –º–æ–¥–µ–ª—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            test_pair_id = cf_rec.pairs['id'].iloc[0]
            recommendations = cf_rec.get_pair_recommendations(test_pair_id, top_k=5)
            
            if not recommendations:
                raise ValueError("–ù–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç CF —Å–∏—Å—Ç–µ–º—ã")
            
            processing_time = time.time() - start_time
            
            self.results['performance_metrics']['cf_latency_ms'] = processing_time * 1000
            self.results['performance_metrics']['cf_recommendations'] = len(recommendations)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã CF —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            sample_recs = [
                {
                    'item_id': rec['item_id'],
                    'rating': rec['combined_rating'],
                    'phase': 'collaborative_filtering'
                }
                for rec in recommendations[:3]
            ]
            self.results['recommendations_samples'].extend(sample_recs)
            
            self.results['passed_tests'].append(f"{phase_name}_training")
            self.results['passed_tests'].append(f"{phase_name}_generation")
            print(f"  ‚úÖ CF —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {len(recommendations)}, –í—Ä–µ–º—è: {processing_time*1000:.2f}ms")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_embeddings(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç Embeddings + ANN (–§–∞–∑–∞ 6)"""
        phase_name = "embeddings"
        self.results['phases_tested'].append(phase_name)
        
        try:
            start_time = time.time()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Embedding —Å–µ—Ä–≤–∏—Å
            embedding_service = EmbeddingService()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if not embedding_service.load_embeddings():
                print("  üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
                embedding_service.generate_user_embeddings()
                embedding_service.generate_product_embeddings()
                embedding_service.generate_pair_embeddings()
                embedding_service.build_faiss_indexes()
                embedding_service.save_embeddings()
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º ANN –ø–æ–∏—Å–∫
            test_pair_id = list(embedding_service.pair_embeddings.keys())[0]
            candidates = embedding_service.find_similar_products_ann(test_pair_id, top_k=5)
            
            if not candidates:
                raise ValueError("–ù–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç ANN –ø–æ–∏—Å–∫–∞")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            latency_stats = embedding_service.benchmark_search_latency(num_queries=10)
            
            processing_time = time.time() - start_time
            
            self.results['performance_metrics']['embeddings_generation_time'] = processing_time
            self.results['performance_metrics']['ann_p95_latency_ms'] = latency_stats['p95_latency_ms']
            self.results['performance_metrics']['embedding_candidates'] = len(candidates)
            
            # –ü—Ä–∏–º–µ—Ä—ã ANN –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            sample_recs = [
                {
                    'title': candidate['title'],
                    'similarity': candidate['embedding_similarity'],
                    'phase': 'embeddings_ann'
                }
                for candidate in candidates[:3]
            ]
            self.results['recommendations_samples'].extend(sample_recs)
            
            self.results['passed_tests'].append(f"{phase_name}_generation")
            self.results['passed_tests'].append(f"{phase_name}_ann_search")
            self.results['passed_tests'].append(f"{phase_name}_performance")
            print(f"  ‚úÖ ANN –ø–æ–∏—Å–∫: {len(candidates)}, P95: {latency_stats['p95_latency_ms']:.2f}ms")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_learning_to_rank(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç Learning to Rank (–§–∞–∑–∞ 7)"""
        phase_name = "learning_to_rank"
        self.results['phases_tested'].append(phase_name)
        
        try:
            start_time = time.time()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LTR —Å–µ—Ä–≤–∏—Å
            ltr_service = LearningToRankService()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º/–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            if not ltr_service.load_model():
                print("  üîÑ –û–±—É—á–∞–µ–º LTR –º–æ–¥–µ–ª—å...")
                training_data, groups = ltr_service.create_training_dataset(sample_pairs=50)
                if len(training_data) > 0:
                    metrics = ltr_service.train_ranker_model(training_data, groups)
                    ltr_service.save_model()
                    
                    self.results['performance_metrics']['ltr_training_ndcg'] = metrics.get('cv_ndcg_mean', 0.0)
                else:
                    print("  ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LTR, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    return
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            test_candidates = [
                {'item_id': 'test1', 'title': 'Test 1', 'content_score': 0.8, 'cf_score': 0.6, 'price': 1000},
                {'item_id': 'test2', 'title': 'Test 2', 'content_score': 0.6, 'cf_score': 0.8, 'price': 1500},
                {'item_id': 'test3', 'title': 'Test 3', 'content_score': 0.7, 'cf_score': 0.7, 'price': 2000}
            ]
            
            ranked_candidates = ltr_service.rank_candidates("test_pair", test_candidates)
            
            if not ranked_candidates:
                raise ValueError("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º LTR scores
            for candidate in ranked_candidates:
                if 'ltr_score' not in candidate:
                    raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç LTR scores")
            
            processing_time = time.time() - start_time
            
            self.results['performance_metrics']['ltr_ranking_latency_ms'] = processing_time * 1000
            self.results['performance_metrics']['ltr_candidates_ranked'] = len(ranked_candidates)
            
            # Feature importance
            feature_importance = ltr_service.get_feature_importance(top_k=5)
            if feature_importance:
                self.results['performance_metrics']['top_ltr_features'] = list(feature_importance.keys())[:3]
            
            # –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            sample_recs = [
                {
                    'title': candidate['title'],
                    'ltr_score': candidate.get('ltr_score', 0),
                    'phase': 'learning_to_rank'
                }
                for candidate in ranked_candidates[:3]
            ]
            self.results['recommendations_samples'].extend(sample_recs)
            
            self.results['passed_tests'].append(f"{phase_name}_training")
            self.results['passed_tests'].append(f"{phase_name}_ranking")
            self.results['passed_tests'].append(f"{phase_name}_feature_importance")
            print(f"  ‚úÖ LTR —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: {len(ranked_candidates)}, –í—Ä–µ–º—è: {processing_time*1000:.2f}ms")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_llm_explainability(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç LLM + Explainability (–§–∞–∑–∞ 8)"""
        phase_name = "llm_explainability"
        self.results['phases_tested'].append(phase_name)
        
        try:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º LLM Wrapper
            print("  ü§ñ –¢–µ—Å—Ç–∏—Ä—É–µ–º LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
            llm_wrapper = LLMWrapper()
            
            # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            pair_context = {
                'common_interests': ['–∫–æ—Ñ–µ', '–º—É–∑—ã–∫–∞'],
                'budget': '—Å—Ä–µ–¥–Ω–∏–π',
                'love_language': 'quality_time'
            }
            
            recommendation = {
                'title': '–ö–æ—Ñ–µ–π–Ω—è "–¢–µ—Å—Ç"',
                'category': 'cafe',
                'price': 800
            }
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–π
            start_time = time.time()
            scenario_response = llm_wrapper.generate_date_scenario(pair_context, recommendation)
            llm_latency = (time.time() - start_time) * 1000
            
            if not scenario_response.generated_text:
                raise ValueError("–ü—É—Å—Ç–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π –æ—Ç LLM")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
            top_reasons = ['–æ–±—â–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã', '–ø–æ–¥—Ö–æ–¥—è—â–∏–π –±—é–¥–∂–µ—Ç', '—É—é—Ç–Ω–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞']
            explanation_response = llm_wrapper.generate_explanation(recommendation, top_reasons)
            
            if not explanation_response.generated_text:
                raise ValueError("–ü—É—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç LLM")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º Explainability Service
            print("  üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º Explainability...")
            explainer = ExplainabilityService()
            
            test_features = {
                'content_score': 0.8,
                'cf_score': 0.6,
                'embedding_score': 0.7,
                'price': 0.5
            }
            
            explanation = explainer.explain_recommendation("test_pair", "test_item", test_features)
            
            if not explanation.top_factors:
                raise ValueError("–ù–µ—Ç —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –≤ –æ–±—ä—è—Å–Ω–µ–Ω–∏–∏")
            
            self.results['performance_metrics']['llm_scenario_latency_ms'] = llm_latency
            self.results['performance_metrics']['llm_model_used'] = scenario_response.model_used
            self.results['performance_metrics']['explanation_confidence'] = explanation.confidence_score
            
            # –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            self.results['recommendations_samples'].append({
                'scenario_sample': scenario_response.generated_text[:100] + "...",
                'explanation_sample': explanation.human_friendly,
                'phase': 'llm_explainability'
            })
            
            self.results['passed_tests'].append(f"{phase_name}_scenario_generation")
            self.results['passed_tests'].append(f"{phase_name}_explanation_generation")
            self.results['passed_tests'].append(f"{phase_name}_explainability")
            print(f"  ‚úÖ LLM —Å—Ü–µ–Ω–∞—Ä–∏–π: {llm_latency:.2f}ms, Explainability: {explanation.confidence_score:.3f}")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_monitoring_etl(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ + ETL (–§–∞–∑–∞ 9)"""
        phase_name = "monitoring_etl"
        self.results['phases_tested'].append(phase_name)
        
        try:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º Monitoring Service
            print("  üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥...")
            monitor = MonitoringService("test_monitoring.db")
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            monitor.record_metric("test_latency", 100.0, {"service": "test"})
            monitor.record_metric("test_accuracy", 0.85, {"model": "test"})
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
            exp_id = monitor.start_experiment("Test Experiment", 0.1, "test_v1", 1)
            
            # –°–∏–º—É–ª–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            monitor.log_activity("test_pair", "view", {"item_id": "test_item"}, experiment_id=exp_id)
            monitor.log_activity("test_pair", "click", {"item_id": "test_item"}, experiment_id=exp_id)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            business_metrics = monitor.get_business_metrics()
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º feature drift
            drift_score = monitor.detect_feature_drift({"test_feature": 0.5})
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º ETL (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            print("  üîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º ETL...")
            etl = ETLPipeline(monitoring_db="test_monitoring.db")
            
            # –°–∏–º—É–ª–∏—Ä—É–µ–º ETL –ø—Ä–æ—Ü–µ—Å—Å (–±–µ–∑ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)
            new_logs = etl._collect_new_logs()
            
            self.results['performance_metrics']['monitoring_experiments_active'] = len(monitor.experiments)
            self.results['performance_metrics']['monitoring_drift_score'] = drift_score
            self.results['performance_metrics']['etl_logs_collected'] = len(new_logs)
            
            self.results['passed_tests'].append(f"{phase_name}_monitoring")
            self.results['passed_tests'].append(f"{phase_name}_experiments")
            self.results['passed_tests'].append(f"{phase_name}_drift_detection")
            self.results['passed_tests'].append(f"{phase_name}_etl")
            print(f"  ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: {len(monitor.experiments)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, Drift: {drift_score:.3f}")
            
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –±–∞–∑—ã
            if os.path.exists("test_monitoring.db"):
                os.remove("test_monitoring.db")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_ultimate_integration(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç Ultimate –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        phase_name = "ultimate_integration"
        self.results['phases_tested'].append(phase_name)
        
        try:
            start_time = time.time()
            
            print("  üåü –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Ultimate AI Service...")
            service = UltimateAIService()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            status = service.get_system_status()
            ready_components = status['ready_components']
            total_components = status['total_components']
            
            if ready_components < 4:  # –ú–∏–Ω–∏–º—É–º 4 –∏–∑ 6 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                print(f"  ‚ö†Ô∏è –ì–æ—Ç–æ–≤–æ —Ç–æ–ª—å–∫–æ {ready_components}/{total_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º Ultimate —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            test_pair_id = service.content_recommender.pairs['id'].iloc[0]
            
            result = service.get_ultimate_recommendations(
                test_pair_id, 
                top_k=3,
                include_scenarios=True,
                include_explanations=True
            )
            
            if not result['recommendations']:
                raise ValueError("–ù–µ—Ç Ultimate —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
            
            processing_time = time.time() - start_time
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            has_scenarios = bool(result.get('scenarios'))
            has_explanations = bool(result.get('explanations'))
            
            self.results['performance_metrics']['ultimate_latency_ms'] = processing_time * 1000
            self.results['performance_metrics']['ultimate_ready_components'] = f"{ready_components}/{total_components}"
            self.results['performance_metrics']['ultimate_method_used'] = result['metadata']['method_used']
            self.results['performance_metrics']['ultimate_has_scenarios'] = has_scenarios
            self.results['performance_metrics']['ultimate_has_explanations'] = has_explanations
            
            # –ü—Ä–∏–º–µ—Ä—ã Ultimate —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            for i, rec in enumerate(result['recommendations'][:2]):
                sample = {
                    'title': rec['title'],
                    'final_score': rec.get('final_score', 0),
                    'method': rec.get('method', 'unknown'),
                    'phase': 'ultimate_integration'
                }
                
                if has_scenarios and i < len(result.get('scenarios', [])):
                    sample['scenario'] = result['scenarios'][i]['scenario'][:50] + "..."
                
                if has_explanations and i < len(result.get('explanations', [])):
                    sample['explanation'] = result['explanations'][i]['human_explanation'][:50] + "..."
                
                self.results['recommendations_samples'].append(sample)
            
            self.results['passed_tests'].append(f"{phase_name}_initialization")
            self.results['passed_tests'].append(f"{phase_name}_recommendations")
            
            if has_scenarios:
                self.results['passed_tests'].append(f"{phase_name}_scenarios")
            if has_explanations:
                self.results['passed_tests'].append(f"{phase_name}_explanations")
            
            print(f"  ‚úÖ Ultimate: {len(result['recommendations'])} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, " +
                  f"–ú–µ—Ç–æ–¥: {result['metadata']['method_used']}, " +
                  f"–í—Ä–µ–º—è: {processing_time*1000:.2f}ms")
            print(f"      –°—Ü–µ–Ω–∞—Ä–∏–∏: {has_scenarios}, –û–±—ä—è—Å–Ω–µ–Ω–∏—è: {has_explanations}")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _test_performance(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã"""
        phase_name = "performance"
        
        try:
            print("  ‚ö° –ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            service = UltimateAIService()
            test_pairs = service.content_recommender.pairs['id'].head(10).tolist()
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            latencies = []
            errors = 0
            
            for pair_id in test_pairs:
                try:
                    start_time = time.time()
                    result = service.get_ultimate_recommendations(pair_id, top_k=5)
                    latency = (time.time() - start_time) * 1000
                    latencies.append(latency)
                    
                    if not result['recommendations']:
                        errors += 1
                        
                except Exception:
                    errors += 1
            
            if latencies:
                avg_latency = np.mean(latencies)
                p95_latency = np.percentile(latencies, 95)
                error_rate = errors / len(test_pairs)
                
                self.results['performance_metrics']['load_test_avg_latency_ms'] = avg_latency
                self.results['performance_metrics']['load_test_p95_latency_ms'] = p95_latency
                self.results['performance_metrics']['load_test_error_rate'] = error_rate
                self.results['performance_metrics']['load_test_requests'] = len(test_pairs)
                
                # –û—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if avg_latency < 500:
                    self.results['passed_tests'].append(f"{phase_name}_avg_latency")
                if p95_latency < 1000:
                    self.results['passed_tests'].append(f"{phase_name}_p95_latency")
                if error_rate < 0.1:
                    self.results['passed_tests'].append(f"{phase_name}_error_rate")
                
                print(f"  ‚úÖ –ù–∞–≥—Ä—É–∑–∫–∞: {len(test_pairs)} –∑–∞–ø—Ä–æ—Å–æ–≤, " +
                      f"Avg: {avg_latency:.1f}ms, P95: {p95_latency:.1f}ms, " +
                      f"Errors: {error_rate*100:.1f}%")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤ {phase_name}: {e}"
            self.results['failed_tests'].append(error_msg)
            self.results['errors'].append(error_msg)
            print(f"  ‚ùå {error_msg}")
    
    def _generate_final_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç"""
        print("\n" + "="*80)
        print("üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
        print("="*80)
        
        total_tests = len(self.results['passed_tests']) + len(self.results['failed_tests'])
        success_rate = len(self.results['passed_tests']) / max(total_tests, 1) * 100
        
        print(f"üïê –í—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {self.results['total_test_time_minutes']:.1f} –º–∏–Ω—É—Ç")
        print(f"üß™ –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
        print(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ: {len(self.results['passed_tests'])}")
        print(f"‚ùå –ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {len(self.results['failed_tests'])}")
        print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
        
        print(f"\nüéØ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–∑—ã:")
        for phase in self.results['phases_tested']:
            phase_tests = [test for test in self.results['passed_tests'] if test.startswith(phase)]
            phase_failures = [test for test in self.results['failed_tests'] if phase in test]
            status = "‚úÖ" if not phase_failures else "‚ö†Ô∏è" if phase_tests else "‚ùå"
            print(f"  {status} {phase}: {len(phase_tests)} —É—Å–ø–µ—à–Ω–æ, {len(phase_failures)} –æ—à–∏–±–æ–∫")
        
        if self.results['errors']:
            print(f"\n‚ùå –û—à–∏–±–∫–∏:")
            for error in self.results['errors'][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                print(f"  - {error}")
        
        # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print(f"\n‚ö° –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        key_metrics = [
            'ultimate_latency_ms', 'load_test_avg_latency_ms', 
            'content_based_latency_ms', 'cf_latency_ms', 'ann_p95_latency_ms'
        ]
        
        for metric in key_metrics:
            if metric in self.results['performance_metrics']:
                value = self.results['performance_metrics'][metric]
                print(f"  {metric}: {value:.2f}")
        
        # –°—Ç–∞—Ç—É—Å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        ready_components = self.results['performance_metrics'].get('ultimate_ready_components', 'unknown')
        print(f"\nüéØ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã: {ready_components} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        if success_rate >= 90:
            status_emoji = "üü¢"
            status_text = "–û–¢–õ–ò–ß–ù–û - –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ production"
        elif success_rate >= 75:
            status_emoji = "üü°"  
            status_text = "–•–û–†–û–®–û - –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ —Å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–æ—Ä–∞–±–æ—Ç–∫–∞–º–∏"
        elif success_rate >= 50:
            status_emoji = "üü†"
            status_text = "–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û - –¢—Ä–µ–±—É—é—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∏"
        else:
            status_emoji = "üî¥"
            status_text = "–ö–†–ò–¢–ò–ß–ù–û - –°–∏—Å—Ç–µ–º–∞ —Ç—Ä–µ–±—É–µ—Ç —Å–µ—Ä—å–µ–∑–Ω—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"
        
        print(f"\n{status_emoji} –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê: {status_text}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        os.makedirs('test_reports', exist_ok=True)
        filename = f"test_reports/comprehensive_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
        print("="*80)

def main():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç comprehensive —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    test_suite = ComprehensiveTestSuite()
    results = test_suite.run_all_tests()
    
    return results

if __name__ == "__main__":
    main()
