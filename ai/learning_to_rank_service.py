#!/usr/bin/env python3
"""
Learning to Rank Service –¥–ª—è LoveMemory AI
–§–∞–∑–∞ 7: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å LightGBM Ranker

–§—É–Ω–∫—Ü–∏–∏:
- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ training dataset —Å —Ñ–∏—á–∞–º–∏ –∏ –ª–µ–π–±–ª–∞–º–∏
- –û–±—É—á–µ–Ω–∏–µ LightGBM Ranker –º–æ–¥–µ–ª–∏
- –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ –ø–∞—Ä–∞–º
- Feature engineering –∏ importance –∞–Ω–∞–ª–∏–∑
- Production inference –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
"""

import json
import os
import pickle
import time
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GroupKFold
from sklearn.metrics import ndcg_score
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö —Å–∏—Å—Ç–µ–º
from content_recommender import ContentBasedRecommender
from collaborative_filtering import CollaborativeFilteringRecommender
from embedding_service import EmbeddingService

class LearningToRankService:
    """–°–µ—Ä–≤–∏—Å Learning to Rank –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    
    def __init__(self, data_path: str = 'data/synthetic_v1'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LTR —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
        """
        self.data_path = data_path
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Learning to Rank Service...")
        self.content_recommender = ContentBasedRecommender(data_path)
        self.cf_recommender = CollaborativeFilteringRecommender(data_path)
        self.embedding_service = EmbeddingService(data_path)
        
        # LightGBM –º–æ–¥–µ–ª—å
        self.ranker_model = None
        self.feature_names = []
        self.feature_importance = {}
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.lgb_params = {
            'objective': 'lambdarank',
            'metric': 'ndcg',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'min_data_in_leaf': 20,
            'verbose': -1
        }
        
        # –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        os.makedirs('models', exist_ok=True)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._prepare_components()
    
    def _prepare_components(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã"""
        print("‚öôÔ∏è –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã LTR...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º CF –º–æ–¥–µ–ª—å
        if os.path.exists('models/cf_svd_v1.pkl'):
            self.cf_recommender.load_model('models/cf_svd_v1')
        else:
            self.cf_recommender.train_svd_model()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        if not self.embedding_service.load_embeddings():
            print("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è LTR...")
            self.embedding_service.generate_user_embeddings()
            self.embedding_service.generate_product_embeddings()
            self.embedding_service.generate_pair_embeddings()
            self.embedding_service.build_faiss_indexes()
            self.embedding_service.save_embeddings()
        
        print("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã LTR –≥–æ—Ç–æ–≤—ã")
    
    def create_training_dataset(self, sample_pairs: int = 500) -> Tuple[pd.DataFrame, List[int]]:
        """
        –°–æ–∑–¥–∞–µ—Ç training dataset –¥–ª—è Learning to Rank
        
        Args:
            sample_pairs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Returns:
            (features_df, groups) - —Ñ–∏—á–∏ –∏ –≥—Ä—É–ø–ø—ã –¥–ª—è LightGBM
        """
        print(f"üìä –°–æ–∑–¥–∞–µ–º training dataset –¥–ª—è {sample_pairs} –ø–∞—Ä...")
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        all_pairs = self.content_recommender.pairs['id'].tolist()
        sample_pair_ids = np.random.choice(all_pairs, min(sample_pairs, len(all_pairs)), replace=False)
        
        training_data = []
        groups = []  # –†–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
        
        for pair_idx, pair_id in enumerate(sample_pair_ids):
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                candidates = self._get_candidates_for_pair(pair_id)
                
                if len(candidates) == 0:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –ª–µ–π–±–ª—ã (—Ä–µ–π—Ç–∏–Ω–≥–∏) –¥–ª—è —ç—Ç–æ–π –ø–∞—Ä—ã
                labels = self._get_labels_for_pair(pair_id, candidates)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                for candidate in candidates:
                    item_id = candidate['item_id']
                    
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ñ–∏—á–∏
                    features = self._extract_features(pair_id, item_id, candidate)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ª–µ–π–±–ª (—Ä–µ–π—Ç–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π)
                    label = labels.get(item_id, 0)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ dataset
                    row = {
                        'pair_id': pair_id,
                        'item_id': item_id,
                        'label': label,
                        **features
                    }
                    training_data.append(row)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã
                groups.append(len(candidates))
                
                if pair_idx % 100 == 0:
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–∞—Ä: {pair_idx + 1}/{len(sample_pair_ids)}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã {pair_id}: {e}")
                continue
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame
        features_df = pd.DataFrame(training_data)
        
        print(f"‚úÖ Training dataset —Å–æ–∑–¥–∞–Ω:")
        print(f"  - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(features_df)}")
        print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø (–ø–∞—Ä): {len(groups)}")
        print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á: {len(features_df.columns) - 3}")  # -3 –¥–ª—è pair_id, item_id, label
        
        return features_df, groups
    
    def _get_candidates_for_pair(self, pair_id: str, max_candidates: int = 50) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        candidates = {}
        
        # Content-based –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        try:
            content_recs = self.content_recommender.recommend_date(pair_id, top_k=20)
            for rec in content_recs:
                candidates[rec.item_id] = {
                    'item_id': rec.item_id,
                    'title': rec.title,
                    'category': rec.category,
                    'price': rec.price,
                    'content_score': rec.score
                }
        except:
            pass
        
        # CF –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        try:
            cf_recs = self.cf_recommender.get_pair_recommendations(pair_id, top_k=20)
            for rec in cf_recs:
                item_id = rec['item_id']
                if item_id in candidates:
                    candidates[item_id]['cf_score'] = rec['combined_rating'] / 10.0
                else:
                    product_info = self._get_product_info(item_id)
                    candidates[item_id] = {
                        'item_id': item_id,
                        'title': product_info.get('title', 'Unknown'),
                        'category': product_info.get('category', 'unknown'),
                        'price': product_info.get('price', 0),
                        'content_score': 0.0,
                        'cf_score': rec['combined_rating'] / 10.0
                    }
        except:
            pass
        
        # Embedding –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        try:
            embedding_recs = self.embedding_service.find_similar_products_ann(pair_id, top_k=20)
            for rec in embedding_recs:
                item_id = rec['item_id']
                if item_id in candidates:
                    candidates[item_id]['embedding_score'] = rec['embedding_similarity']
                else:
                    candidates[item_id] = {
                        'item_id': item_id,
                        'title': rec['title'],
                        'category': rec['category'],
                        'price': rec['price'],
                        'content_score': 0.0,
                        'cf_score': 0.0,
                        'embedding_score': rec['embedding_similarity']
                    }
        except:
            pass
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ scores
        for candidate in candidates.values():
            candidate.setdefault('content_score', 0.0)
            candidate.setdefault('cf_score', 0.0)
            candidate.setdefault('embedding_score', 0.0)
        
        return list(candidates.values())[:max_candidates]
    
    def _get_labels_for_pair(self, pair_id: str, candidates: List[Dict]) -> Dict[str, float]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ª–µ–π–±–ª—ã (—Ä–µ–π—Ç–∏–Ω–≥–∏) –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        labels = {}
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–∞—Ä—ã
            pair = self.content_recommender.pairs[self.content_recommender.pairs['id'] == pair_id].iloc[0]
            user1_id = pair['user1_id']
            user2_id = pair['user2_id']
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            user_interactions = self.content_recommender.interactions[
                (self.content_recommender.interactions['user_id'] == user1_id) |
                (self.content_recommender.interactions['user_id'] == user2_id)
            ]
            
            # –°–æ–∑–¥–∞–µ–º –ª–µ–π–±–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
            for _, interaction in user_interactions.iterrows():
                product_id = interaction['product_id']
                rating = interaction['rating']
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –ª–µ–π–±–ª—ã –∏–ª–∏ relevance scores
                if rating >= 8:
                    relevance = 3  # –û—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                elif rating >= 6:
                    relevance = 2  # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                elif rating >= 4:
                    relevance = 1  # –°–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                else:
                    relevance = 0  # –ù–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
                
                # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥, –µ—Å–ª–∏ –æ–±–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ—Ü–µ–Ω–∏–ª–∏
                if product_id in labels:
                    labels[product_id] = max(labels[product_id], relevance)
                else:
                    labels[product_id] = relevance
            
            # –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –±–µ–∑ —è–≤–Ω—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ª–µ–π–±–ª—ã
            # –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–µ—Ç–∏–ø–æ–≤ –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π
            for candidate in candidates:
                item_id = candidate['item_id']
                if item_id not in labels:
                    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –ª–µ–π–±–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ scores
                    synthetic_score = (
                        candidate['content_score'] * 0.4 +
                        candidate['cf_score'] * 0.3 +
                        candidate['embedding_score'] * 0.3
                    )
                    
                    if synthetic_score >= 0.7:
                        labels[item_id] = 2
                    elif synthetic_score >= 0.5:
                        labels[item_id] = 1
                    else:
                        labels[item_id] = 0
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ª–µ–π–±–ª–æ–≤ –¥–ª—è –ø–∞—Ä—ã {pair_id}: {e}")
        
        return labels
    
    def _extract_features(self, pair_id: str, item_id: str, candidate: Dict) -> Dict[str, float]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Ñ–∏—á–∏ –¥–ª—è –ø–∞—Ä—ã item
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            item_id: ID —Ç–æ–≤–∞—Ä–∞
            candidate: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞–Ω–¥–∏–¥–∞—Ç–µ
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ñ–∏—á–∞–º–∏
        """
        features = {}
        
        try:
            # 1. –ë–∞–∑–æ–≤—ã–µ scores –æ—Ç —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            features['content_score'] = candidate.get('content_score', 0.0)
            features['cf_score'] = candidate.get('cf_score', 0.0)
            features['embedding_score'] = candidate.get('embedding_score', 0.0)
            
            # 2. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ
            product_info = self._get_product_info(item_id)
            features['price'] = product_info.get('price', 0) / 1000.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ü–µ–Ω—É
            features['price_log'] = np.log1p(product_info.get('price', 1))
            
            # 3. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ (one-hot encoding)
            category = product_info.get('category', 'unknown')
            features['is_restaurant'] = 1.0 if category == 'restaurant' else 0.0
            features['is_cafe'] = 1.0 if category == 'cafe' else 0.0
            features['is_entertainment'] = 1.0 if category == 'entertainment' else 0.0
            features['is_gift'] = 1.0 if category == 'gift' else 0.0
            
            # 4. –§–∏—á–∏ –ø–∞—Ä—ã
            pair_features = self._get_pair_features(pair_id)
            features.update(pair_features)
            
            # 5. –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
            features['day_of_week'] = datetime.now().weekday()
            features['is_weekend'] = 1.0 if datetime.now().weekday() >= 5 else 0.0
            
            # 6. –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–∞
            popularity = self._get_item_popularity(item_id)
            features['item_popularity'] = popularity
            features['item_popularity_log'] = np.log1p(popularity)
            
            # 7. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏
            features['content_cf_product'] = features['content_score'] * features['cf_score']
            features['content_embedding_product'] = features['content_score'] * features['embedding_score']
            features['cf_embedding_product'] = features['cf_score'] * features['embedding_score']
            
            # 8. –¶–µ–Ω–∞ vs –∞—Ä—Ö–µ—Ç–∏–ø match
            features['price_match_score'] = self._calculate_price_match(pair_id, product_info.get('price', 0))
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏—á –¥–ª—è {pair_id}, {item_id}: {e}")
        
        return features
    
    def _get_product_info(self, item_id: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ"""
        try:
            product = self.content_recommender.product_catalog[
                self.content_recommender.product_catalog['id'] == item_id
            ]
            if not product.empty:
                product = product.iloc[0]
                return product.to_dict()
        except:
            pass
        return {}
    
    def _get_pair_features(self, pair_id: str) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏—á–∏ –ø–∞—Ä—ã"""
        features = {}
        
        try:
            pair = self.content_recommender.pairs[self.content_recommender.pairs['id'] == pair_id].iloc[0]
            user1 = self.content_recommender.users[self.content_recommender.users['id'] == pair['user1_id']].iloc[0]
            user2 = self.content_recommender.users[self.content_recommender.users['id'] == pair['user2_id']].iloc[0]
            
            # –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ —Ñ–∏—á–∏
            features['age_diff'] = abs(user1['age'] - user2['age']) / 50.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            features['avg_age'] = (user1['age'] + user2['age']) / 2.0 / 50.0
            
            # –ê—Ä—Ö–µ—Ç–∏–ø—ã
            features['same_archetype'] = 1.0 if user1['archetype'] == user2['archetype'] else 0.0
            
            # –ê—Ä—Ö–µ—Ç–∏–ø—ã –∫–∞–∫ —Ñ–∏—á–∏
            archetype1 = user1['archetype']
            archetype2 = user2['archetype']
            for archetype in ['ArtLovers', 'Gamers', 'Gourmets', 'Fitness', 'Travelers']:
                features[f'user1_{archetype.lower()}'] = 1.0 if archetype1 == archetype else 0.0
                features[f'user2_{archetype.lower()}'] = 1.0 if archetype2 == archetype else 0.0
            
            # –ë—é–¥–∂–µ—Ç–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
            budget_match = 1.0 if user1['budget_preference'] == user2['budget_preference'] else 0.0
            features['budget_match'] = budget_match
            
            # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            features['user1_activity'] = user1.get('activity_probability', 0.5)
            features['user2_activity'] = user2.get('activity_probability', 0.5)
            features['avg_activity'] = (features['user1_activity'] + features['user2_activity']) / 2.0
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏—á –ø–∞—Ä—ã {pair_id}: {e}")
        
        return features
    
    def _get_item_popularity(self, item_id: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–∞"""
        try:
            item_interactions = self.content_recommender.interactions[
                self.content_recommender.interactions['product_id'] == item_id
            ]
            return len(item_interactions) / len(self.content_recommender.interactions)
        except:
            return 0.0
    
    def _calculate_price_match(self, pair_id: str, price: float) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ü–µ–Ω—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–∞—Ä—ã"""
        try:
            pair = self.content_recommender.pairs[self.content_recommender.pairs['id'] == pair_id].iloc[0]
            user1 = self.content_recommender.users[self.content_recommender.users['id'] == pair['user1_id']].iloc[0]
            user2 = self.content_recommender.users[self.content_recommender.users['id'] == pair['user2_id']].iloc[0]
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ budget_preference
            budget_map = {'low': 1000, 'medium': 2500, 'high': 5000}
            
            budget1 = budget_map.get(user1['budget_preference'], 2500)
            budget2 = budget_map.get(user2['budget_preference'], 2500)
            avg_budget = (budget1 + budget2) / 2.0
            
            # –ß–µ–º –±–ª–∏–∂–µ —Ü–µ–Ω–∞ –∫ –±—é–¥–∂–µ—Ç—É, —Ç–µ–º –≤—ã—à–µ score
            if price <= avg_budget:
                return price / avg_budget
            else:
                return max(0.1, avg_budget / price)
                
        except:
            return 0.5
    
    def train_ranker_model(self, training_data: pd.DataFrame, groups: List[int], 
                          validation_split: float = 0.2, n_folds: int = 3) -> Dict:
        """
        –û–±—É—á–∞–µ—Ç LightGBM Ranker –º–æ–¥–µ–ª—å —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        
        Args:
            training_data: Training dataset
            groups: –†–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            n_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        print("üöÄ –û–±—É—á–∞–µ–º LightGBM Ranker –º–æ–¥–µ–ª—å...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        feature_cols = [col for col in training_data.columns 
                       if col not in ['pair_id', 'item_id', 'label']]
        self.feature_names = feature_cols
        
        X = training_data[feature_cols].values
        y = training_data['label'].values
        
        print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
        print(f"  - –§–∏—á–∏: {X.shape}")
        print(f"  - –õ–µ–π–±–ª—ã: {y.shape}")
        print(f"  - –ì—Ä—É–ø–ø—ã: {len(groups)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        group_kfold = GroupKFold(n_splits=n_folds)
        cv_scores = []
        feature_importances = []
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä—É–ø–ø—ã –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (pair_id –∫–∞–∫ –≥—Ä—É–ø–ø–∞)
        pair_groups = training_data['pair_id'].values
        
        fold = 0
        for train_idx, val_idx in group_kfold.split(X, y, groups=pair_groups):
            fold += 1
            print(f"  üìä –§–æ–ª–¥ {fold}/{n_folds}")
            
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # –ì—Ä—É–ø–ø—ã –¥–ª—è train –∏ val
            train_groups = [groups[i] for i in range(len(groups)) if any(idx in train_idx for idx in range(sum(groups[:i]), sum(groups[:i+1])))]
            val_groups = [groups[i] for i in range(len(groups)) if any(idx in val_idx for idx in range(sum(groups[:i]), sum(groups[:i+1])))]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)
            val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)
            
            model = lgb.train(
                self.lgb_params,
                train_data,
                valid_sets=[val_data],
                num_boost_round=1000,
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
            )
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            y_pred = model.predict(X_val)
            
            # –í—ã—á–∏—Å–ª—è–µ–º NDCG@10 –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –≤ validation
            ndcg_scores = []
            start_idx = 0
            for group_size in val_groups:
                end_idx = start_idx + group_size
                if group_size > 1:  # NDCG —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 2 —ç–ª–µ–º–µ–Ω—Ç–∞
                    group_true = y_val[start_idx:end_idx]
                    group_pred = y_pred[start_idx:end_idx]
                    
                    # Reshape –¥–ª—è NDCG
                    ndcg = ndcg_score([group_true], [group_pred], k=min(10, group_size))
                    ndcg_scores.append(ndcg)
                
                start_idx = end_idx
            
            if ndcg_scores:
                fold_ndcg = np.mean(ndcg_scores)
                cv_scores.append(fold_ndcg)
                print(f"    NDCG@10: {fold_ndcg:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º feature importance
            importance = model.feature_importance(importance_type='gain')
            feature_importances.append(importance)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        print("üéØ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
        train_data = lgb.Dataset(X, label=y, group=groups)
        
        self.ranker_model = lgb.train(
            self.lgb_params,
            train_data,
            num_boost_round=1000,
            callbacks=[lgb.log_evaluation(100)]
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º feature importance
        if feature_importances:
            avg_importance = np.mean(feature_importances, axis=0)
            self.feature_importance = dict(zip(self.feature_names, avg_importance))
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = {
            'cv_ndcg_mean': np.mean(cv_scores) if cv_scores else 0.0,
            'cv_ndcg_std': np.std(cv_scores) if cv_scores else 0.0,
            'n_folds': len(cv_scores),
            'feature_count': len(self.feature_names),
            'training_samples': len(training_data),
            'training_groups': len(groups)
        }
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        print(f"  CV NDCG@10: {metrics['cv_ndcg_mean']:.4f} ¬± {metrics['cv_ndcg_std']:.4f}")
        print(f"  –§–∏—á–∏: {metrics['feature_count']}")
        
        return metrics
    
    def rank_candidates(self, pair_id: str, candidates: List[Dict]) -> List[Dict]:
        """
        –†–∞–Ω–∂–∏—Ä—É–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            candidates: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        """
        if self.ranker_model is None:
            print("‚ö†Ô∏è Ranker –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return candidates
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            features_list = []
            for candidate in candidates:
                features = self._extract_features(pair_id, candidate['item_id'], candidate)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ñ–∏—á–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
                feature_vector = [features.get(name, 0.0) for name in self.feature_names]
                features_list.append(feature_vector)
            
            if not features_list:
                return candidates
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º relevance scores
            X = np.array(features_list)
            scores = self.ranker_model.predict(X)
            
            # –î–æ–±–∞–≤–ª—è–µ–º scores –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            for i, candidate in enumerate(candidates):
                candidate['ltr_score'] = scores[i]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ LTR score
            ranked_candidates = sorted(candidates, key=lambda x: x['ltr_score'], reverse=True)
            
            return ranked_candidates
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–∞—Ä—ã {pair_id}: {e}")
            return candidates
    
    def get_feature_importance(self, top_k: int = 20) -> Dict[str, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á"""
        if not self.feature_importance:
            return {}
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        sorted_features = sorted(self.feature_importance.items(), 
                               key=lambda x: x[1], reverse=True)
        
        return dict(sorted_features[:top_k])
    
    def save_model(self, model_path: str = 'models/ltr_v1'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç LTR –º–æ–¥–µ–ª—å"""
        if self.ranker_model is None:
            print("‚ö†Ô∏è –ù–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º LightGBM –º–æ–¥–µ–ª—å
        self.ranker_model.save_model(f'{model_path}.txt')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_id': 'ltr_v1',
            'type': 'learning_to_rank',
            'version': '1.0',
            'algorithm': 'lightgbm_ranker',
            'feature_names': self.feature_names,
            'feature_importance': self.feature_importance,
            'lgb_params': self.lgb_params,
            'created_at': datetime.now().isoformat()
        }
        
        with open(f'{model_path}_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ LTR –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    def load_model(self, model_path: str = 'models/ltr_v1'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç LTR –º–æ–¥–µ–ª—å"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º LightGBM –º–æ–¥–µ–ª—å
            self.ranker_model = lgb.Booster(model_file=f'{model_path}.txt')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(f'{model_path}_metadata.json', 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            self.feature_names = metadata['feature_names']
            self.feature_importance = metadata['feature_importance']
            
            print(f"‚úÖ LTR –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LTR –º–æ–¥–µ–ª—å: {e}")
            return False

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Learning to Rank Service"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Learning to Rank Service")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å
    ltr_service = LearningToRankService()
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
    if not ltr_service.load_model():
        print("üîÑ –û–±—É—á–∞–µ–º –Ω–æ–≤—É—é LTR –º–æ–¥–µ–ª—å...")
        
        # –°–æ–∑–¥–∞–µ–º training dataset
        training_data, groups = ltr_service.create_training_dataset(sample_pairs=200)
        
        if len(training_data) == 0:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å training dataset")
            return
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        metrics = ltr_service.train_ranker_model(training_data, groups)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        ltr_service.save_model()
        
        print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è LTR:")
        print(f"  CV NDCG@10: {metrics['cv_ndcg_mean']:.4f} ¬± {metrics['cv_ndcg_std']:.4f}")
        print(f"  –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {metrics['training_samples']}")
        print(f"  –ì—Ä—É–ø–ø (–ø–∞—Ä): {metrics['training_groups']}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
    test_pair_id = ltr_service.content_recommender.pairs['id'].iloc[0]
    print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º LTR —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–∞—Ä—ã: {test_pair_id}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    candidates = ltr_service._get_candidates_for_pair(test_pair_id, max_candidates=10)
    
    if candidates:
        # –†–∞–Ω–∂–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        ranked_candidates = ltr_service.rank_candidates(test_pair_id, candidates)
        
        print(f"\nüìã –¢–æ–ø-5 LTR —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
        for i, candidate in enumerate(ranked_candidates[:5], 1):
            print(f"{i}. {candidate['title']}")
            print(f"   LTR Score: {candidate.get('ltr_score', 0):.4f}")
            print(f"   Content: {candidate.get('content_score', 0):.3f} | CF: {candidate.get('cf_score', 0):.3f} | Embedding: {candidate.get('embedding_score', 0):.3f}")
            print(f"   –¶–µ–Ω–∞: {candidate['price']} —Ä—É–±.")
            print()
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á
    feature_importance = ltr_service.get_feature_importance(top_k=10)
    if feature_importance:
        print(f"\nüîç –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö —Ñ–∏—á:")
        for i, (feature, importance) in enumerate(feature_importance.items(), 1):
            print(f"{i:2d}. {feature}: {importance:.3f}")
    
    print("\nüéâ Learning to Rank —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞!")
    print("‚úÖ –§–∞–∑–∞ 7 (Learning to Rank) –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main()
