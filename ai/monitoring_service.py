#!/usr/bin/env python3
"""
Monitoring Service –¥–ª—è LoveMemory AI
–§–∞–∑–∞ 9: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

–§—É–Ω–∫—Ü–∏–∏:
- –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (latency, error rate, throughput)
- A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å experiment_id  
- Feature drift detection
- Business –º–µ—Ç—Ä–∏–∫–∏ (CTR, acceptance rate)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- Canary deployment –¥–ª—è –º–æ–¥–µ–ª–µ–π
"""

import json
import time
import os
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from dataclasses import dataclass, asdict
import pickle
import hashlib
import sqlite3
from collections import defaultdict, deque
import warnings
warnings.filterwarnings('ignore')

@dataclass
class MetricRecord:
    """–ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫–∏"""
    timestamp: float
    metric_name: str
    value: float
    labels: Dict[str, str]
    experiment_id: Optional[str] = None

@dataclass
class AlertConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–∞"""
    metric_name: str
    threshold: float
    comparison: str  # "gt", "lt", "eq"
    window_minutes: int = 5
    min_samples: int = 10

@dataclass
class ExperimentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
    experiment_id: str
    name: str
    traffic_split: float  # 0.0-1.0
    model_version: str
    start_time: datetime
    end_time: Optional[datetime] = None
    status: str = "running"  # running, stopped, completed

class MonitoringService:
    """–°–µ—Ä–≤–∏—Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ AI —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, db_path: str = "monitoring.db"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Monitoring Service
        
        Args:
            db_path: –ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        """
        self.db_path = db_path
        self.metrics_buffer = deque(maxlen=10000)  # –ë—É—Ñ–µ—Ä –¥–ª—è –º–µ—Ç—Ä–∏–∫
        self.experiments = {}  # –ê–∫—Ç–∏–≤–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
        self.alerts = []  # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–ª–µ—Ä—Ç–æ–≤
        self.feature_baselines = {}  # –ë–∞–∑–æ–≤—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∏—á
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        self._init_database()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–ª–µ—Ä—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self._setup_default_alerts()
        
        print("üìä Monitoring Service –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç SQLite –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        with sqlite3.connect(self.db_path) as conn:
            # –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫
            conn.execute("""
                CREATE TABLE IF NOT EXISTS metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL NOT NULL,
                    metric_name TEXT NOT NULL,
                    value REAL NOT NULL,
                    labels TEXT,
                    experiment_id TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # –¢–∞–±–ª–∏—Ü–∞ –ª–æ–≥–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            conn.execute("""
                CREATE TABLE IF NOT EXISTS activity_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pair_id TEXT NOT NULL,
                    user_id TEXT,
                    action TEXT NOT NULL,
                    payload TEXT,
                    model_version TEXT,
                    experiment_id TEXT,
                    timestamp REAL NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # –¢–∞–±–ª–∏—Ü–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
            conn.execute("""
                CREATE TABLE IF NOT EXISTS experiments (
                    experiment_id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    traffic_split REAL NOT NULL,
                    model_version TEXT NOT NULL,
                    start_time DATETIME NOT NULL,
                    end_time DATETIME,
                    status TEXT DEFAULT 'running',
                    config TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            conn.execute("CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON metrics(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_metrics_name ON metrics(metric_name)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_activity_timestamp ON activity_logs(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_experiments_status ON experiments(status)")
    
    def _setup_default_alerts(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∞–ª–µ—Ä—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        self.alerts = [
            AlertConfig("latency_p95", 500.0, "gt", window_minutes=5),
            AlertConfig("error_rate", 0.05, "gt", window_minutes=5),
            AlertConfig("throughput", 0.1, "lt", window_minutes=10),
            AlertConfig("model_accuracy", 0.6, "lt", window_minutes=30),
            AlertConfig("feature_drift_score", 0.3, "gt", window_minutes=60)
        ]
    
    def record_metric(self, metric_name: str, value: float, 
                     labels: Optional[Dict[str, str]] = None,
                     experiment_id: Optional[str] = None):
        """
        –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É
        
        Args:
            metric_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            value: –ó–Ω–∞—á–µ–Ω–∏–µ
            labels: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ª–µ–π–±–ª—ã
            experiment_id: ID —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        """
        record = MetricRecord(
            timestamp=time.time(),
            metric_name=metric_name,
            value=value,
            labels=labels or {},
            experiment_id=experiment_id
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä
        self.metrics_buffer.append(record)
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
        if len(self.metrics_buffer) % 100 == 0:
            self._flush_metrics_to_db()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–ª–µ—Ä—Ç—ã
        self._check_alerts(record)
    
    def _flush_metrics_to_db(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –±—É—Ñ–µ—Ä–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        if not self.metrics_buffer:
            return
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                records_to_insert = []
                
                while self.metrics_buffer:
                    record = self.metrics_buffer.popleft()
                    records_to_insert.append((
                        record.timestamp,
                        record.metric_name,
                        record.value,
                        json.dumps(record.labels),
                        record.experiment_id
                    ))
                
                conn.executemany("""
                    INSERT INTO metrics (timestamp, metric_name, value, labels, experiment_id)
                    VALUES (?, ?, ?, ?, ?)
                """, records_to_insert)
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")
    
    def _check_alerts(self, record: MetricRecord):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞–ª–µ—Ä—Ç—ã –¥–ª—è –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏"""
        for alert in self.alerts:
            if record.metric_name == alert.metric_name:
                self._evaluate_alert(alert, record)
    
    def _evaluate_alert(self, alert: AlertConfig, record: MetricRecord):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∞–ª–µ—Ä—Ç"""
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏
        recent_values = self._get_recent_metric_values(
            alert.metric_name, 
            minutes=alert.window_minutes
        )
        
        if len(recent_values) < alert.min_samples:
            return
        
        avg_value = np.mean(recent_values)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏–µ –∞–ª–µ—Ä—Ç–∞
        triggered = False
        if alert.comparison == "gt" and avg_value > alert.threshold:
            triggered = True
        elif alert.comparison == "lt" and avg_value < alert.threshold:
            triggered = True
        elif alert.comparison == "eq" and abs(avg_value - alert.threshold) < 0.01:
            triggered = True
        
        if triggered:
            self._trigger_alert(alert, avg_value, recent_values)
    
    def _get_recent_metric_values(self, metric_name: str, minutes: int = 5) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–µ–¥–∞–≤–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏"""
        cutoff_time = time.time() - (minutes * 60)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT value FROM metrics 
                    WHERE metric_name = ? AND timestamp > ?
                    ORDER BY timestamp DESC
                """, (metric_name, cutoff_time))
                
                return [row[0] for row in cursor.fetchall()]
        except:
            return []
    
    def _trigger_alert(self, alert: AlertConfig, avg_value: float, recent_values: List[float]):
        """–°—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞–ª–µ—Ä—Ç"""
        print(f"üö® ALERT: {alert.metric_name}")
        print(f"   Threshold: {alert.threshold}, Current: {avg_value:.3f}")
        print(f"   Window: {alert.window_minutes}min, Samples: {len(recent_values)}")
        
        # –í production –∑–¥–µ—Å—å –±—ã–ª –±—ã –æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ Slack/email
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∞–ª–µ—Ä—Ç –∫–∞–∫ –º–µ—Ç—Ä–∏–∫—É
        self.record_metric(
            f"alert_{alert.metric_name}",
            1.0,
            labels={"threshold": str(alert.threshold), "current_value": str(avg_value)}
        )
    
    def log_activity(self, pair_id: str, action: str, payload: Dict[str, Any],
                    user_id: Optional[str] = None, model_version: str = "v1",
                    experiment_id: Optional[str] = None):
        """
        –õ–æ–≥–∏—Ä—É–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
            action: –î–µ–π—Å—Ç–≤–∏–µ (click, view, accept, reject)
            payload: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è  
            model_version: –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏
            experiment_id: ID —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT INTO activity_logs 
                    (pair_id, user_id, action, payload, model_version, experiment_id, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    pair_id, user_id, action, 
                    json.dumps(payload), model_version, 
                    experiment_id, time.time()
                ))
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {e}")
    
    def start_experiment(self, name: str, traffic_split: float, 
                        model_version: str, duration_hours: int = 24) -> str:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç A/B —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        
        Args:
            name: –ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            traffic_split: –î–æ–ª—è —Ç—Ä–∞—Ñ–∏–∫–∞ (0.0-1.0)
            model_version: –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            duration_hours: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —á–∞—Å–∞—Ö
        
        Returns:
            ID —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        """
        experiment_id = f"exp_{int(time.time())}_{hashlib.md5(name.encode()).hexdigest()[:8]}"
        
        end_time = datetime.now() + timedelta(hours=duration_hours)
        
        experiment = ExperimentConfig(
            experiment_id=experiment_id,
            name=name,
            traffic_split=traffic_split,
            model_version=model_version,
            start_time=datetime.now(),
            end_time=end_time
        )
        
        self.experiments[experiment_id] = experiment
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT INTO experiments 
                    (experiment_id, name, traffic_split, model_version, start_time, end_time, config)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    experiment_id, name, traffic_split, model_version,
                    experiment.start_time.isoformat(), experiment.end_time.isoformat(),
                    json.dumps(asdict(experiment))
                ))
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {e}")
        
        print(f"üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–ø—É—â–µ–Ω: {experiment_id}")
        print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {name}")
        print(f"   –¢—Ä–∞—Ñ–∏–∫: {traffic_split*100:.1f}%")
        print(f"   –ú–æ–¥–µ–ª—å: {model_version}")
        
        return experiment_id
    
    def should_use_experiment(self, pair_id: str) -> Optional[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –¥–æ–ª–∂–µ–Ω –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–ø–∞—Å—Ç—å –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        
        Args:
            pair_id: ID –ø–∞—Ä—ã
        
        Returns:
            ID —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏–ª–∏ None
        """
        for exp_id, experiment in self.experiments.items():
            if experiment.status != "running":
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
            if experiment.end_time and datetime.now() > experiment.end_time:
                experiment.status = "completed"
                continue
            
            # –•—ç—à–∏—Ä—É–µ–º pair_id –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            hash_value = int(hashlib.md5(f"{pair_id}_{exp_id}".encode()).hexdigest()[:8], 16)
            normalized_hash = (hash_value % 10000) / 10000.0
            
            if normalized_hash < experiment.traffic_split:
                return exp_id
        
        return None
    
    def get_experiment_metrics(self, experiment_id: str, 
                             days: int = 7) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        
        Args:
            experiment_id: ID —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        """
        cutoff_time = time.time() - (days * 24 * 3600)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                exp_metrics = {}
                cursor = conn.execute("""
                    SELECT metric_name, AVG(value), COUNT(*), MIN(value), MAX(value)
                    FROM metrics 
                    WHERE experiment_id = ? AND timestamp > ?
                    GROUP BY metric_name
                """, (experiment_id, cutoff_time))
                
                for row in cursor:
                    metric_name, avg_val, count, min_val, max_val = row
                    exp_metrics[metric_name] = {
                        'avg': avg_val,
                        'count': count,
                        'min': min_val,
                        'max': max_val
                    }
                
                # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ
                cursor = conn.execute("""
                    SELECT action, COUNT(*) 
                    FROM activity_logs 
                    WHERE experiment_id = ? AND timestamp > ?
                    GROUP BY action
                """, (experiment_id, cutoff_time))
                
                user_actions = dict(cursor.fetchall())
                
                # CTR –∏ conversion rate
                clicks = user_actions.get('click', 0)
                views = user_actions.get('view', 0)
                accepts = user_actions.get('accept', 0)
                
                ctr = clicks / max(views, 1)
                conversion_rate = accepts / max(clicks, 1)
                
                return {
                    'experiment_id': experiment_id,
                    'metrics': exp_metrics,
                    'user_actions': user_actions,
                    'ctr': ctr,
                    'conversion_rate': conversion_rate,
                    'total_users': len(set([]))  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
                }
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {e}")
            return {}
    
    def detect_feature_drift(self, current_features: Dict[str, float],
                           model_name: str = "default") -> float:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ç drift –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ñ–∏—á
        
        Args:
            current_features: –¢–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ–∏—á
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Returns:
            Drift score (0-1, –≥–¥–µ 1 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π drift)
        """
        baseline_key = f"{model_name}_baseline"
        
        # –ï—Å–ª–∏ –Ω–µ—Ç baseline, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ
        if baseline_key not in self.feature_baselines:
            self.feature_baselines[baseline_key] = defaultdict(list)
            for feature, value in current_features.items():
                self.feature_baselines[baseline_key][feature].append(value)
            return 0.0
        
        baseline = self.feature_baselines[baseline_key]
        drift_scores = []
        
        for feature, current_value in current_features.items():
            if feature in baseline and len(baseline[feature]) > 10:
                baseline_values = np.array(baseline[feature])
                baseline_mean = np.mean(baseline_values)
                baseline_std = np.std(baseline_values)
                
                # Z-score drift
                if baseline_std > 0:
                    z_score = abs(current_value - baseline_mean) / baseline_std
                    drift_score = min(1.0, z_score / 3.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                    drift_scores.append(drift_score)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º baseline (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ)
            baseline[feature].append(current_value)
            if len(baseline[feature]) > 1000:
                baseline[feature] = baseline[feature][-500:]  # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 500
        
        overall_drift = np.mean(drift_scores) if drift_scores else 0.0
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫—É drift
        self.record_metric("feature_drift_score", overall_drift, 
                          labels={"model": model_name})
        
        return overall_drift
    
    def get_business_metrics(self, days: int = 7) -> Dict[str, float]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
        
        Args:
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –ë–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
        """
        cutoff_time = time.time() - (days * 24 * 3600)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–µ–π—Å—Ç–≤–∏—è
                cursor = conn.execute("""
                    SELECT action, COUNT(*) 
                    FROM activity_logs 
                    WHERE timestamp > ?
                    GROUP BY action
                """, (cutoff_time,))
                
                actions = dict(cursor.fetchall())
                
                views = actions.get('view', 0)
                clicks = actions.get('click', 0) 
                accepts = actions.get('accept', 0)
                rejects = actions.get('reject', 0)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                ctr = clicks / max(views, 1)
                acceptance_rate = accepts / max(clicks, 1)
                rejection_rate = rejects / max(clicks, 1)
                engagement_rate = (clicks + accepts) / max(views, 1)
                
                # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã
                cursor = conn.execute("""
                    SELECT COUNT(DISTINCT pair_id) 
                    FROM activity_logs 
                    WHERE timestamp > ?
                """, (cutoff_time,))
                
                unique_pairs = cursor.fetchone()[0]
                
                return {
                    'ctr': ctr,
                    'acceptance_rate': acceptance_rate,
                    'rejection_rate': rejection_rate,
                    'engagement_rate': engagement_rate,
                    'total_views': views,
                    'total_clicks': clicks,
                    'total_accepts': accepts,
                    'unique_pairs': unique_pairs,
                    'days_analyzed': days
                }
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫: {e}")
            return {}
    
    def generate_daily_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç"""
        print("üìà –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç...")
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        business_metrics = self.get_business_metrics(days=1)
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        tech_metrics = {}
        for metric_name in ['latency_p95', 'error_rate', 'throughput']:
            values = self._get_recent_metric_values(metric_name, minutes=24*60)
            if values:
                tech_metrics[metric_name] = {
                    'avg': np.mean(values),
                    'p95': np.percentile(values, 95),
                    'count': len(values)
                }
        
        # –ê–∫—Ç–∏–≤–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
        active_experiments = [
            exp_id for exp_id, exp in self.experiments.items() 
            if exp.status == "running"
        ]
        
        report = {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'business_metrics': business_metrics,
            'technical_metrics': tech_metrics,
            'active_experiments': active_experiments,
            'alerts_triggered': len([1 for _ in self._get_recent_metric_values('alert_', minutes=24*60)]),
            'feature_drift_avg': np.mean(self._get_recent_metric_values('feature_drift_score', minutes=24*60)) or 0.0
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        os.makedirs('reports', exist_ok=True)
        filename = f"reports/daily_report_{datetime.now().strftime('%Y%m%d')}.json"
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)
        
        return report
    
    def cleanup_old_data(self, days_to_keep: int = 30):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        cutoff_time = time.time() - (days_to_keep * 24 * 3600)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                cursor = conn.execute("DELETE FROM metrics WHERE timestamp < ?", (cutoff_time,))
                metrics_deleted = cursor.rowcount
                
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                cursor = conn.execute("DELETE FROM activity_logs WHERE timestamp < ?", (cutoff_time,))
                logs_deleted = cursor.rowcount
                
                print(f"üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: —É–¥–∞–ª–µ–Ω–æ {metrics_deleted} –º–µ—Ç—Ä–∏–∫, {logs_deleted} –ª–æ–≥–æ–≤")
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Monitoring Service"""
    print("üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Monitoring Service")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å
    monitor = MonitoringService()
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
    print("\n1Ô∏è‚É£ –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")
    for i in range(10):
        monitor.record_metric("latency_p95", 100 + i*10, {"service": "ai"})
        monitor.record_metric("error_rate", 0.01 + i*0.001, {"service": "ai"})
        monitor.record_metric("throughput", 50 - i, {"service": "ai"})
        time.sleep(0.1)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
    print("\n2Ô∏è‚É£ –ó–∞–ø—É—Å–∫–∞–µ–º A/B —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç...")
    exp_id = monitor.start_experiment(
        name="New LTR Model Test",
        traffic_split=0.1,
        model_version="ltr_v2",
        duration_hours=1
    )
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    print("\n3Ô∏è‚É£ –õ–æ–≥–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
    test_pairs = [f"pair_{i}" for i in range(5)]
    
    for pair_id in test_pairs:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        experiment_id = monitor.should_use_experiment(pair_id)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏—è
        monitor.log_activity(pair_id, "view", {"item_id": "test_item"}, experiment_id=experiment_id)
        monitor.log_activity(pair_id, "click", {"item_id": "test_item"}, experiment_id=experiment_id)
        
        if np.random.random() > 0.5:
            monitor.log_activity(pair_id, "accept", {"item_id": "test_item"}, experiment_id=experiment_id)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º feature drift
    print("\n4Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º feature drift detection...")
    baseline_features = {"content_score": 0.8, "cf_score": 0.6, "price": 0.5}
    monitor.detect_feature_drift(baseline_features)
    
    # Drift —Å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ–º
    drifted_features = {"content_score": 0.2, "cf_score": 0.9, "price": 0.1}
    drift_score = monitor.detect_feature_drift(drifted_features)
    print(f"   Drift score: {drift_score:.3f}")
    
    # –ü–æ–ª—É—á–∞–µ–º –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
    print("\n5Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏...")
    business_metrics = monitor.get_business_metrics()
    print(f"   CTR: {business_metrics.get('ctr', 0):.3f}")
    print(f"   Acceptance rate: {business_metrics.get('acceptance_rate', 0):.3f}")
    print(f"   Unique pairs: {business_metrics.get('unique_pairs', 0)}")
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    print("\n6Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç...")
    exp_metrics = monitor.get_experiment_metrics(exp_id)
    print(f"   Experiment CTR: {exp_metrics.get('ctr', 0):.3f}")
    print(f"   User actions: {exp_metrics.get('user_actions', {})}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    print("\n7Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á–µ—Ç...")
    report = monitor.generate_daily_report()
    print(f"   –ê–ª–µ—Ä—Ç–æ–≤ –∑–∞ –¥–µ–Ω—å: {report['alerts_triggered']}")
    print(f"   –°—Ä–µ–¥–Ω–∏–π drift: {report['feature_drift_avg']:.3f}")
    
    # –û—á–∏—Å—Ç–∫–∞
    print("\n8Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—á–∏—Å—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö...")
    monitor.cleanup_old_data(days_to_keep=1)
    
    print("\n‚úÖ Monitoring Service –≥–æ—Ç–æ–≤ –∫ production!")

if __name__ == "__main__":
    main()
