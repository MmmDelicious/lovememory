#!/usr/bin/env python3
"""
LLM Wrapper –¥–ª—è LoveMemory AI
–§–∞–∑–∞ 8: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å–≤–∏–¥–∞–Ω–∏–π –∏ explainability

–§—É–Ω–∫—Ü–∏–∏:
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥—Ä–æ–±–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å–≤–∏–¥–∞–Ω–∏–π
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–¥–∞—Ä–æ—á–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π  
- –û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–∫–æ—Ä–æ—Ç–∫–∏–µ bullet points)
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å TTL –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- Rate limiting –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π
- –ó–∞—â–∏—Ç–∞ –æ—Ç hallucination
"""

import json
import time
import hashlib
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import os
from dataclasses import dataclass
import pickle
import re

# –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º template-based –≥–µ–Ω–µ—Ä–∞—Ü–∏—é")

@dataclass
class LLMRequest:
    """–ó–∞–ø—Ä–æ—Å –∫ LLM"""
    prompt: str
    max_length: int = 500
    temperature: float = 0.7
    top_p: float = 0.9
    request_type: str = "general"  # scenario, gift_text, explanation

@dataclass
class LLMResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç LLM"""
    generated_text: str
    cached: bool = False
    processing_time_ms: float = 0.0
    model_used: str = "unknown"
    tokens_used: int = 0

class LLMCache:
    """–ü—Ä–æ—Å—Ç–æ–π –∫—ç—à –¥–ª—è LLM –æ—Ç–≤–µ—Ç–æ–≤"""
    
    def __init__(self, cache_file: str = "llm_cache.pkl", ttl_hours: int = 24):
        self.cache_file = cache_file
        self.ttl_seconds = ttl_hours * 3600
        self.cache = self._load_cache()
        
    def _load_cache(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à —Å –¥–∏—Å–∫–∞"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
        return {}
    
    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –Ω–∞ –¥–∏—Å–∫"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def get(self, prompt_hash: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞"""
        if prompt_hash in self.cache:
            response, timestamp = self.cache[prompt_hash]
            if time.time() - timestamp < self.ttl_seconds:
                return response
            else:
                del self.cache[prompt_hash]
        return None
    
    def set(self, prompt_hash: str, response: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –∫—ç—à"""
        self.cache[prompt_hash] = (response, time.time())
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
        if len(self.cache) % 100 == 0:
            self._cleanup_expired()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø–∏—Å–µ–π
        if len(self.cache) % 10 == 0:
            self._save_cache()
    
    def _cleanup_expired(self):
        """–£–¥–∞–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.cache.items()
            if current_time - timestamp >= self.ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]

class RateLimiter:
    """Rate limiter –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π"""
    
    def __init__(self, max_requests: int = 100, window_minutes: int = 60):
        self.max_requests = max_requests
        self.window_seconds = window_minutes * 60
        self.requests = {}  # {client_id: [timestamps]}
    
    def is_allowed(self, client_id: str = "default") -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ä–∞–∑—Ä–µ—à–µ–Ω –ª–∏ –∑–∞–ø—Ä–æ—Å"""
        current_time = time.time()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–ª–∏ –æ—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if client_id not in self.requests:
            self.requests[client_id] = []
        else:
            # –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –≤–Ω–µ –æ–∫–Ω–∞
            cutoff_time = current_time - self.window_seconds
            self.requests[client_id] = [
                timestamp for timestamp in self.requests[client_id]
                if timestamp > cutoff_time
            ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
        if len(self.requests[client_id]) >= self.max_requests:
            return False
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
        self.requests[client_id].append(current_time)
        return True

class LLMWrapper:
    """Wrapper –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium", use_local: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Wrapper
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            use_local: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ API
        """
        self.model_name = model_name
        self.use_local = use_local and TRANSFORMERS_AVAILABLE
        
        # –ö—ç—à –∏ rate limiting
        self.cache = LLMCache()
        self.rate_limiter = RateLimiter(max_requests=50, window_minutes=60)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'rate_limited': 0,
            'errors': 0,
            'total_tokens': 0
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        self.model = None
        self.tokenizer = None
        self._initialize_model()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞"""
        if self.use_local:
            try:
                print(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {self.model_name}")
                
                # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
                self.tokenizer = AutoTokenizer.from_pretrained(
                    "microsoft/DialoGPT-small",
                    padding_side='left'
                )
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                print("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
                print("üìù –ò—Å–ø–æ–ª—å–∑—É–µ–º template-based –≥–µ–Ω–µ—Ä–∞—Ü–∏—é")
                self.use_local = False
    
    def _generate_hash(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö—ç—à –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        return hashlib.md5(prompt.encode()).hexdigest()
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text.strip())
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        sentences = text.split('.')
        unique_sentences = []
        seen = set()
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in seen and len(sentence) > 10:
                unique_sentences.append(sentence)
                seen.add(sentence)
        
        return '. '.join(unique_sentences) + '.' if unique_sentences else text
    
    def _validate_output(self, text: str, request_type: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        if not text or len(text.strip()) < 20:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ hallucination (–±–∞–∑–æ–≤–∞—è)
        hallucination_markers = [
            "—Ç–æ—á–Ω—ã–π –∞–¥—Ä–µ—Å:", "—Ç–µ–ª–µ—Ñ–æ–Ω:", "www.", "http://", "https://",
            "—Å—Ç–æ–∏–º–æ—Å—Ç—å:", "—Ü–µ–Ω–∞ —Ç–æ—á–Ω–æ", "—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–æ"
        ]
        
        text_lower = text.lower()
        if any(marker in text_lower for marker in hallucination_markers):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞
        if request_type == "scenario" and len(text.split('.')) < 3:
            return False
        
        if request_type == "explanation" and not any(word in text_lower for word in ['–ø–æ—Ç–æ–º—É', '—Ç–∞–∫ –∫–∞–∫', '–∏–∑-–∑–∞', '–±–ª–∞–≥–æ–¥–∞—Ä—è']):
            return False
        
        return True
    
    def generate_date_scenario(self, pair_context: Dict, recommendation: Dict) -> LLMResponse:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π —Å–≤–∏–¥–∞–Ω–∏—è
        
        Args:
            pair_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞—Ä—ã (–∏–Ω—Ç–µ—Ä–µ—Å—ã, –±—é–¥–∂–µ—Ç, –∞—Ä—Ö–µ—Ç–∏–ø—ã)
            recommendation: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–π
        
        Returns:
            –ü–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π —Å–≤–∏–¥–∞–Ω–∏—è
        """
        prompt = self._build_scenario_prompt(pair_context, recommendation)
        
        request = LLMRequest(
            prompt=prompt,
            max_length=400,
            temperature=0.8,
            request_type="scenario"
        )
        
        return self._generate(request)
    
    def generate_gift_description(self, pair_context: Dict, gift_item: Dict) -> LLMResponse:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–¥–∞—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Args:
            pair_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞—Ä—ã
            gift_item: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–¥–∞—Ä–∫–µ
        
        Returns:
            –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–¥–∞—Ä–∫–∞
        """
        prompt = self._build_gift_prompt(pair_context, gift_item)
        
        request = LLMRequest(
            prompt=prompt,
            max_length=200,
            temperature=0.7,
            request_type="gift_text"
        )
        
        return self._generate(request)
    
    def generate_explanation(self, recommendation: Dict, top_reasons: List[str]) -> LLMResponse:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ—á–µ–º—É –±—ã–ª–∞ –¥–∞–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        
        Args:
            recommendation: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
            top_reasons: –¢–æ–ø –ø—Ä–∏—á–∏–Ω—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        
        Returns:
            –ö—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤ –≤–∏–¥–µ bullet points
        """
        prompt = self._build_explanation_prompt(recommendation, top_reasons)
        
        request = LLMRequest(
            prompt=prompt,
            max_length=150,
            temperature=0.6,
            request_type="explanation"
        )
        
        return self._generate(request)
    
    def _build_scenario_prompt(self, pair_context: Dict, recommendation: Dict) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        interests = pair_context.get('common_interests', [])
        budget = pair_context.get('budget', '—Å—Ä–µ–¥–Ω–∏–π')
        location_type = recommendation.get('category', '–º–µ—Å—Ç–æ')
        title = recommendation.get('title', '–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å')
        
        prompt = f"""–°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–æ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–π —Å–≤–∏–¥–∞–Ω–∏—è –¥–ª—è –ø–∞—Ä—ã.

–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞—Ä—ã:
- –û–±—â–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã: {', '.join(interests[:3])}
- –ë—é–¥–∂–µ—Ç: {budget}
- –ú–µ—Å—Ç–æ: {title} ({location_type})

–°—Ü–µ–Ω–∞—Ä–∏–π –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å:
1. –ö–∞–∫ –ª—É—á—à–µ –¥–æ–±—Ä–∞—Ç—å—Å—è –∏ –∫–æ–≥–¥–∞ –ø—Ä–∏–π—Ç–∏
2. –ß—Ç–æ –¥–µ–ª–∞—Ç—å –∏ –≤ –∫–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ  
3. –û —á–µ–º –ø–æ–≥–æ–≤–æ—Ä–∏—Ç—å
4. –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å –æ—Å–æ–±—É—é –∞—Ç–º–æ—Å—Ñ–µ—Ä—É

–°—Ü–µ–Ω–∞—Ä–∏–π (200-300 —Å–ª–æ–≤):"""

        return prompt
    
    def _build_gift_prompt(self, pair_context: Dict, gift_item: Dict) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –ø–æ–¥–∞—Ä–∫–∞"""
        partner_interests = pair_context.get('partner_interests', [])
        love_language = pair_context.get('love_language', '–Ω–µ —É–∫–∞–∑–∞–Ω')
        gift_title = gift_item.get('title', '–ø–æ–¥–∞—Ä–æ–∫')
        
        prompt = f"""–°–æ–∑–¥–∞–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–¥–∞—Ä–∫–∞ –¥–ª—è –ª—é–±–∏–º–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞.

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—É—á–∞—Ç–µ–ª–µ:
- –ò–Ω—Ç–µ—Ä–µ—Å—ã: {', '.join(partner_interests[:3])}
- –Ø–∑—ã–∫ –ª—é–±–≤–∏: {love_language}
- –ü–æ–¥–∞—Ä–æ–∫: {gift_title}

–û–ø–∏—à–∏ –ø–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –ø–æ–¥–∞—Ä–æ–∫ –∏–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥–æ–π–¥–µ—Ç, –∫–∞–∫ –µ–≥–æ –ø—Ä–µ–ø–æ–¥–Ω–µ—Å—Ç–∏ –∏ —á—Ç–æ –æ–Ω —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É–µ—Ç.

–û–ø–∏—Å–∞–Ω–∏–µ (100-150 —Å–ª–æ–≤):"""

        return prompt
    
    def _build_explanation_prompt(self, recommendation: Dict, top_reasons: List[str]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"""
        title = recommendation.get('title', '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è')
        
        prompt = f"""–û–±—ä—è—Å–Ω–∏ –∫—Ä–∞—Ç–∫–æ –ø–æ—á–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω "{title}".

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã:
{chr(10).join(f'- {reason}' for reason in top_reasons[:3])}

–°–æ–∑–¥–∞–π 2-3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ bullet points, –∫–∞–∂–¥–æ–µ –Ω–µ –±–æ–ª–µ–µ 10 —Å–ª–æ–≤.

–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:"""

        return prompt
    
    def _generate(self, request: LLMRequest) -> LLMResponse:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        start_time = time.time()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats['total_requests'] += 1
        
        # Rate limiting
        if not self.rate_limiter.is_allowed():
            self.stats['rate_limited'] += 1
            return LLMResponse(
                generated_text="–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used="rate_limited"
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        prompt_hash = self._generate_hash(request.prompt)
        cached_response = self.cache.get(prompt_hash)
        
        if cached_response:
            self.stats['cache_hits'] += 1
            return LLMResponse(
                generated_text=cached_response,
                cached=True,
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used="cache"
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        try:
            if self.use_local and self.tokenizer:
                generated_text = self._generate_local(request)
                model_used = "local_" + self.model_name
            else:
                generated_text = self._generate_template_based(request)
                model_used = "template_based"
            
            # –û—á–∏—â–∞–µ–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º
            generated_text = self._clean_text(generated_text)
            
            if not self._validate_output(generated_text, request.request_type):
                # Fallback –Ω–∞ template-based
                generated_text = self._generate_template_based(request)
                model_used = "template_fallback"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self.cache.set(prompt_hash, generated_text)
            
            processing_time = (time.time() - start_time) * 1000
            
            return LLMResponse(
                generated_text=generated_text,
                processing_time_ms=processing_time,
                model_used=model_used,
                tokens_used=len(generated_text.split())
            )
            
        except Exception as e:
            self.stats['errors'] += 1
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            
            # Fallback –Ω–∞ —à–∞–±–ª–æ–Ω—ã
            fallback_text = self._generate_template_based(request)
            
            return LLMResponse(
                generated_text=fallback_text,
                processing_time_ms=(time.time() - start_time) * 1000,
                model_used="template_fallback"
            )
    
    def _generate_local(self, request: LLMRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ tokenizer (–±–µ–∑ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤)
        # –í production –∑–¥–µ—Å—å –±—ã–ª –±—ã –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π inference
        
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º template-based –ø–æ–¥—Ö–æ–¥
        return self._generate_template_based(request)
    
    def _generate_template_based(self, request: LLMRequest) -> str:
        """Template-based –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–∫ fallback"""
        
        if request.request_type == "scenario":
            return self._generate_scenario_template(request.prompt)
        elif request.request_type == "gift_text":
            return self._generate_gift_template(request.prompt)
        elif request.request_type == "explanation":
            return self._generate_explanation_template(request.prompt)
        else:
            return "–°—Ü–µ–Ω–∞—Ä–∏–π –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤."
    
    def _generate_scenario_template(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ü–µ–Ω–∞—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–æ–≤"""
        templates = [
            "–ü—Ä–∏—Ö–æ–¥–∏—Ç–µ –∑–∞ 15 –º–∏–Ω—É—Ç –¥–æ –≤—Å—Ç—Ä–µ—á–∏, —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–µ–µ –º–µ—Å—Ç–æ. –ù–∞—á–Ω–∏—Ç–µ —Å –ª–µ–≥–∫–æ–π –±–µ—Å–µ–¥—ã –æ –≤–∞—à–∏—Ö –æ–±—â–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–∞—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –º–µ—Å—Ç–∞ –∏ –ø–æ–¥–µ–ª–∏—Ç–µ—Å—å –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è–º–∏. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Ñ–æ—Ç–æ –Ω–∞ –ø–∞–º—è—Ç—å. –ó–∞–≤–µ—Ä—à–∏—Ç–µ –≤—Å—Ç—Ä–µ—á—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–≤–∏–¥–∞–Ω–∏—è.",
            
            "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø—Ä–∏–π—Ç–∏ –Ω–µ–º–Ω–æ–≥–æ —Ä–∞–Ω—å—à–µ, —á—Ç–æ–±—ã –æ—Å–≤–æ–∏—Ç—å—Å—è –≤ –æ–±—Å—Ç–∞–Ω–æ–≤–∫–µ. –ù–∞—á–Ω–∏—Ç–µ —Ä–∞–∑–≥–æ–≤–æ—Ä —Å —Ç–æ–≥–æ, —á—Ç–æ –≤–∞—Å –ø—Ä–∏–≤–µ–ª–æ –≤ —ç—Ç–æ –º–µ—Å—Ç–æ. –ü–æ–¥–µ–ª–∏—Ç–µ—Å—å —Å–≤–æ–∏–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏ –∏ —É–∑–Ω–∞–π—Ç–µ –º–Ω–µ–Ω–∏–µ –ø–∞—Ä—Ç–Ω–µ—Ä–∞. –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–π –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã. –ù–µ –∑–∞–±—É–¥—å—Ç–µ –≤—ã—Ä–∞–∑–∏—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å –∑–∞ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è.",
            
            "–í—ã–±–µ—Ä–∏—Ç–µ –≤—Ä–µ–º—è, –∫–æ–≥–¥–∞ —É –≤–∞—Å –æ–±–æ–∏—Ö –±—É–¥–µ—Ç —Ö–æ—Ä–æ—à–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ. –ù–∞—á–Ω–∏—Ç–µ —Å –æ–±—Å—É–∂–¥–µ–Ω–∏—è —Ç–æ–≥–æ, —á—Ç–æ –≤–∞—Å –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –≤–ø–µ—á–∞—Ç–ª—è–µ—Ç –≤ —ç—Ç–æ–º –º–µ—Å—Ç–µ. –î–µ–ª–∏—Ç–µ—Å—å —ç–º–æ—Ü–∏—è–º–∏ –∏ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è–º–∏ –ø–æ —Ö–æ–¥—É –≤—Å—Ç—Ä–µ—á–∏. –°–æ–∑–¥–∞–π—Ç–µ –æ—Å–æ–±—ã–µ –º–æ–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –¥–µ—Ç–∞–ª—è–º. –ó–∞–≤–µ—Ä—à–∏—Ç–µ –æ–±–º–µ–Ω –ø–ª–∞–Ω–∞–º–∏ –Ω–∞ –±—É–¥—É—â–∏–µ —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
        ]
        
        return templates[hash(prompt) % len(templates)]
    
    def _generate_gift_template(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–¥–∞—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–æ–≤"""
        templates = [
            "–≠—Ç–æ—Ç –ø–æ–¥–∞—Ä–æ–∫ –∏–¥–µ–∞–ª—å–Ω–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –≤–∞—à–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º –ø–∞—Ä—Ç–Ω–µ—Ä–∞. –û–Ω –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–ª—É—à–∞–µ—Ç–µ –∏ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç–µ —Ç–æ, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ª—é–±–∏–º–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞. –ü—Ä–µ–ø–æ–¥–Ω–µ—Å–∏—Ç–µ –µ–≥–æ –≤ –æ—Å–æ–±–µ–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç, –æ–±—ä—è—Å–Ω–∏–≤, –ø–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –≤—ã–±–æ—Ä.",
            
            "–î–∞–Ω–Ω—ã–π –ø–æ–¥–∞—Ä–æ–∫ —Å—Ç–∞–Ω–µ—Ç –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–º –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –æ –≤–∞—à–∏—Ö —á—É–≤—Å—Ç–≤–∞—Ö. –û–Ω —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å –∏ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –°–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é —Ä–∞—Å—Å–∫–∞–∑–æ–º –æ —Ç–æ–º, –∫–∞–∫ –¥–æ–ª–≥–æ –≤—ã –≤—ã–±–∏—Ä–∞–ª–∏ —á—Ç–æ-—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ–µ –∏–º–µ–Ω–Ω–æ –¥–ª—è –Ω–µ–≥–æ/–Ω–µ—ë.",
            
            "–≠—Ç–æ—Ç –≤—ã–±–æ—Ä –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≥–ª—É–±–∏–Ω—É –≤–∞—à–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π –ø–∞—Ä—Ç–Ω–µ—Ä–∞. –ü–æ–¥–∞—Ä–æ–∫ –±—É–¥–µ—Ç –ø—Ä–∏–Ω–æ—Å–∏—Ç—å —Ä–∞–¥–æ—Å—Ç—å –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –∏ –Ω–∞–ø–æ–º–∏–Ω–∞—Ç—å –æ –≤–∞—à–µ–π –∑–∞–±–æ—Ç–µ. –ü—Ä–µ–ø–æ–¥–Ω–µ—Å–∏—Ç–µ –µ–≥–æ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º —Ç–æ–≥–æ, –∫–∞–∫ –æ–Ω —Å–≤—è–∑–∞–Ω —Å –≤–∞—à–∏–º–∏ –æ–±—â–∏–º–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏."
        ]
        
        return templates[hash(prompt) % len(templates)]
    
    def _generate_explanation_template(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–æ–≤"""
        explanations = [
            "‚Ä¢ –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—à–∏–º –æ–±—â–∏–º –∏–Ω—Ç–µ—Ä–µ—Å–∞–º\n‚Ä¢ –ü–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ –∑–∞—è–≤–ª–µ–Ω–Ω—ã–π –±—é–¥–∂–µ—Ç\n‚Ä¢ –ù–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —É–¥–æ–±–Ω–æ–π –ª–æ–∫–∞—Ü–∏–∏",
            "‚Ä¢ –ü–æ–ø—É–ª—è—Ä–Ω–æ —Å—Ä–µ–¥–∏ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä\n‚Ä¢ –í—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n‚Ä¢ –ü–æ–¥—Ö–æ–¥—è—â–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞ –¥–ª—è –≤–∞—Å",
            "‚Ä¢ –ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –≤–∞—à–µ–≥–æ –∞—Ä—Ö–µ—Ç–∏–ø–∞\n‚Ä¢ –£—á–∏—Ç—ã–≤–∞–µ—Ç —è–∑—ã–∫–∏ –ª—é–±–≤–∏\n‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –ø–æ—Å–µ—â–µ–Ω–∏—è"
        ]
        
        return explanations[hash(prompt) % len(explanations)]
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        return {
            **self.stats,
            'cache_hit_rate': self.stats['cache_hits'] / max(self.stats['total_requests'], 1),
            'cache_size': len(self.cache.cache),
            'model_available': self.use_local
        }

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã LLM Wrapper"""
    print("ü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM Wrapper")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º wrapper
    llm = LLMWrapper()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    pair_context = {
        'common_interests': ['–∫–æ—Ñ–µ', '–∏—Å–∫—É—Å—Å—Ç–≤–æ', '–º—É–∑—ã–∫–∞'],
        'budget': '—Å—Ä–µ–¥–Ω–∏–π',
        'partner_interests': ['—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è', '–∫–Ω–∏–≥–∏'],
        'love_language': '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è'
    }
    
    recommendation = {
        'title': '–ö–æ—Ñ–µ–π–Ω—è "–ê—Ä–æ–º–∞—Ç"',
        'category': 'cafe',
        'price': 800
    }
    
    # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è
    print("\n1Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ü–µ–Ω–∞—Ä–∏—è —Å–≤–∏–¥–∞–Ω–∏—è:")
    scenario_response = llm.generate_date_scenario(pair_context, recommendation)
    print(f"–ú–æ–¥–µ–ª—å: {scenario_response.model_used}")
    print(f"–í—Ä–µ–º—è: {scenario_response.processing_time_ms:.2f}ms")
    print(f"–ö—ç—à: {scenario_response.cached}")
    print(f"–°—Ü–µ–Ω–∞—Ä–∏–π:\n{scenario_response.generated_text}")
    
    # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è –ø–æ–¥–∞—Ä–∫–∞
    print("\n2Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –ø–æ–¥–∞—Ä–∫–∞:")
    gift_item = {'title': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–Ω–∏–≥–∞'}
    gift_response = llm.generate_gift_description(pair_context, gift_item)
    print(f"–û–ø–∏—Å–∞–Ω–∏–µ:\n{gift_response.generated_text}")
    
    # –¢–µ—Å—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
    print("\n3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è:")
    top_reasons = ['–æ–±—â–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∫ –∫–æ—Ñ–µ', '–ø–æ–¥—Ö–æ–¥—è—â–∏–π –±—é–¥–∂–µ—Ç', '—É—é—Ç–Ω–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞']
    explanation_response = llm.generate_explanation(recommendation, top_reasons)
    print(f"–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:\n{explanation_response.generated_text}")
    
    # –¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å)
    print("\n4Ô∏è‚É£ –¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è:")
    cached_response = llm.generate_date_scenario(pair_context, recommendation)
    print(f"–ö—ç—à: {cached_response.cached}")
    print(f"–í—Ä–µ–º—è: {cached_response.processing_time_ms:.2f}ms")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n5Ô∏è‚É£ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    stats = llm.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\n‚úÖ LLM Wrapper –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")

if __name__ == "__main__":
    main()
